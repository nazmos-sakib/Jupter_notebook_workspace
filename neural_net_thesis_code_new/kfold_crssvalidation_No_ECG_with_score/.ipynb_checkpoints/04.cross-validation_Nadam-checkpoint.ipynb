{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import all necessary liberty\n",
    "\n",
    "%matplotlib inline\n",
    "#import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#missing value handle\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "#To shuffle the data set\n",
    "from sklearn.utils import shuffle\n",
    "#spliting dataset into traning set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras.optimizers import SGD, Adagrad, Adadelta, RMSprop, Adam\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Risk score\n",
    "\n",
    "def risk__score_calculate(data):\n",
    "    risk_score = 0\n",
    "    \n",
    "    # 0\n",
    "    #age\n",
    "    #15-30=1\n",
    "    #30-40=2\n",
    "    #40-55=3\n",
    "    #above >= 55 =4\n",
    "    \n",
    "    if(int(data[0])>=15 and int(data[0])<30):\n",
    "        risk_score = risk_score + 1\n",
    "        \n",
    "    elif(int(data[0])>=30 and int(data[0])<40):\n",
    "        risk_score = risk_score + 2\n",
    "        \n",
    "    elif(int(data[0])>=40 and int(data[0])<55):\n",
    "        risk_score = risk_score + 3\n",
    "        \n",
    "    elif(int(data[0])>=55):\n",
    "        risk_score = risk_score + 4\n",
    "    \n",
    "    #print(risk_score)\n",
    "    \n",
    "    # 1\n",
    "    #gender\n",
    "    #male-1=4\n",
    "    #female-2=2\n",
    "    \n",
    "    if(int(data[1])==1):\n",
    "        risk_score = risk_score + 4\n",
    "        \n",
    "    elif(int(data[1])==2):\n",
    "        risk_score = risk_score + 2\n",
    "    \n",
    "    #print(risk_score)\n",
    "          \n",
    "    # 2\n",
    "    #smoking\n",
    "    #yes\" : 1  =4\n",
    "    #\"no\" : 0  =1\n",
    "    #\"ex\" : 2  =3\n",
    "    \n",
    "    if(int(data[2])==1):\n",
    "        risk_score = risk_score + 4\n",
    "        \n",
    "    elif(int(data[2])==0):\n",
    "        risk_score = risk_score + 1\n",
    "    \n",
    "    elif(int(data[2])==2):\n",
    "        risk_score = risk_score + 3\n",
    "        \n",
    "    #print(risk_score)\n",
    "    \n",
    "    # 3\n",
    "    #'HTN', \n",
    "    #\"yes\" : 1, =4\n",
    "    #\"no\" : 0, =2\n",
    "    \n",
    "    if(int(data[3])==1):\n",
    "        risk_score = risk_score + 4\n",
    "        \n",
    "    elif(int(data[3])==0):\n",
    "        risk_score = risk_score + 2\n",
    "    \n",
    "    #print(risk_score)\n",
    "    \n",
    "    # 4\n",
    "    #'DLP', \n",
    "    #\"yes\" : 1, =4\n",
    "    #\"no\" : 0, =2\n",
    "    \n",
    "    if(int(data[4])==1):\n",
    "        risk_score = risk_score + 4\n",
    "        \n",
    "    elif(int(data[4])==0):\n",
    "        risk_score = risk_score + 2\n",
    "    \n",
    "    #print(risk_score)\n",
    "    \n",
    "    # 5\n",
    "    #'DM', \n",
    "    #\"yes\" : 1, =4\n",
    "    #\"no\" : 0, =2\n",
    "    \n",
    "    if(int(data[5])==1):\n",
    "        risk_score = risk_score + 4\n",
    "        \n",
    "    elif(int(data[5])==0):\n",
    "        risk_score = risk_score + 2\n",
    "    \n",
    "    #print(risk_score)\n",
    "    \n",
    "    # 6\n",
    "    #'Physical Exercise',\n",
    "    #\"yes\" : 1, =4\n",
    "    #\"no\" : 0, =2\n",
    "    \n",
    "    if(int(data[6])==1):\n",
    "        risk_score = risk_score + 4\n",
    "        \n",
    "    elif(int(data[6])==0):\n",
    "        risk_score = risk_score + 2\n",
    "    \n",
    "    #print(risk_score)\n",
    "    \n",
    "    # 7\n",
    "    #'Family History', \n",
    "    #\"yes\" : 1, =4\n",
    "    #\"no\" : 0, =2\n",
    "    \n",
    "    if(int(data[7])==1):\n",
    "        risk_score = risk_score + 4\n",
    "        \n",
    "    elif(int(data[7])==0):\n",
    "        risk_score = risk_score + 2\n",
    "    \n",
    "    #print(risk_score)\n",
    "    \n",
    "    # 8\n",
    "    #'Drug History', \n",
    "    #\"yes\" : 1, =4\n",
    "    #\"no\" : 0, =2\n",
    "    \n",
    "    if(int(data[8])==1):\n",
    "        risk_score = risk_score + 4\n",
    "        \n",
    "    elif(int(data[8])==0):\n",
    "        risk_score = risk_score + 2\n",
    "    \n",
    "    #print(risk_score)\n",
    "    \n",
    "    # 9\n",
    "    #'Psychological Stress', \n",
    "    #\"yes\" : 1, =4\n",
    "    #\"no\" : 0, =2\n",
    "    \n",
    "    if(int(data[9])==1):\n",
    "        risk_score = risk_score + 4\n",
    "        \n",
    "    elif(int(data[9])==0):\n",
    "        risk_score = risk_score + 2\n",
    "    \n",
    "    #print(risk_score)\n",
    "    \n",
    "    # 10\n",
    "    #'Chest Pain',\n",
    "    #\"yes\" : 1, =4\n",
    "    #\"no\" : 0, =2\n",
    "    \n",
    "    if(int(data[10])==1):\n",
    "        risk_score = risk_score + 4\n",
    "        \n",
    "    elif(int(data[10])==0):\n",
    "        risk_score = risk_score + 2\n",
    "    \n",
    "    #print(risk_score)\n",
    "    \n",
    "    # 11\n",
    "    #'Dyspnea', \n",
    "    #\"yes\" : 1, =4\n",
    "    #\"no\" : 0, =2\n",
    "    \n",
    "    if(int(data[11])==1):\n",
    "        risk_score = risk_score + 4\n",
    "        \n",
    "    elif(int(data[11])==0):\n",
    "        risk_score = risk_score + 2\n",
    "    \n",
    "    #print(risk_score)\n",
    "    \n",
    "    # 12\n",
    "    #'Palpitation', \n",
    "    #\"yes\" : 1, =4\n",
    "    #\"no\" : 0, =2\n",
    "    \n",
    "    if(int(data[12])==1):\n",
    "        risk_score = risk_score + 4\n",
    "        \n",
    "    elif(int(data[12])==0):\n",
    "        risk_score = risk_score + 2\n",
    "    \n",
    "    #print(risk_score)\n",
    "    \n",
    "    '''\n",
    "    # 13\n",
    "    #'ECG',\n",
    "    #\"abnormal\" : 1, =4\n",
    "    #\"normal\" : 0, =2\n",
    "    \n",
    "    if(int(data[13])==1):\n",
    "        risk_score = risk_score + 4\n",
    "        \n",
    "    elif(int(data[13])==0):\n",
    "        risk_score = risk_score + 2\n",
    "    '''\n",
    "    \n",
    "    return risk_score\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used function\n",
    "\n",
    "\n",
    "def read_csv_798():\n",
    "    data_frame_798 = pd.read_csv('data_798.tab', sep='\\t')\n",
    "    yes = []\n",
    "    no = []\n",
    "    for i in range(len(data_frame_798)):\n",
    "        if(int(data_frame_798[\"1.0.7\"][i])==0):\n",
    "            yes.append(0)\n",
    "            no.append(1)\n",
    "            \n",
    "        if(int(data_frame_798[\"1.0.7\"][i])==1):\n",
    "            yes.append(1)\n",
    "            no.append(0)\n",
    "    \n",
    "    data_frame_798['yes']=yes\n",
    "    data_frame_798['no']=no\n",
    "    \n",
    "    print(len(data_frame_798))\n",
    "    print(len(yes))\n",
    "    print(len(no))\n",
    "    return data_frame_798\n",
    "\n",
    "def read_csv_834():\n",
    "    data_frame_834 = pd.read_csv('data_834.tab', sep='\\t')\n",
    "    yes = []\n",
    "    no = []\n",
    "    for i in range(len(data_frame_834)):\n",
    "        if(int(data_frame_834['1.0.4'][i])==0):\n",
    "            yes.append(0)\n",
    "            no.append(1)\n",
    "            \n",
    "        if(int(data_frame_834['1.0.4'][i])==1):\n",
    "            yes.append(1)\n",
    "            no.append(0)\n",
    "    \n",
    "    data_frame_834['yes']=yes\n",
    "    data_frame_834['no']=no\n",
    "    \n",
    "    print(len(data_frame_834))\n",
    "    print(len(yes))\n",
    "    print(len(no))\n",
    "    return data_frame_834\n",
    "\n",
    "def read_csv_835():\n",
    "    data_frame_m = pd.read_csv('RihanSirPreprocess.tab', sep='\\t')\n",
    "    yes = []\n",
    "    no = []\n",
    "    data_frame_m.drop([\"Risk Type\",\"low\",\"medium\",\"high\"],axis=1,inplace=True)\n",
    "    data_frame_m = data_frame_m[['Age', 'Sex', 'Smoking', 'HTN', 'DLP', 'DM', 'Physical Exercise',\n",
    "       'Family History', 'Drug History', 'Psychological Stress', 'Chest Pain',\n",
    "       'Dyspnea', 'Palpitation', 'ECG','Risk Score', 'IHD-HeartAttack']]\n",
    "    for i in range(len(data_frame_m)):\n",
    "        if(int(data_frame_m['IHD-HeartAttack'][i])==0):\n",
    "            yes.append(0)\n",
    "            no.append(1)\n",
    "            \n",
    "        if(int(data_frame_m['IHD-HeartAttack'][i])==1):\n",
    "            yes.append(1)\n",
    "            no.append(0)\n",
    "    \n",
    "    data_frame_m['yes']=yes\n",
    "    data_frame_m['no']=no\n",
    "    \n",
    "    print(len(data_frame_m))\n",
    "    print(len(yes))\n",
    "    print(len(no))\n",
    "    return data_frame_m\n",
    "\n",
    "\n",
    "#suffle the data set\n",
    "def create_data_set(data_frame):\n",
    "    df = data_frame.reindex(np.random.permutation(data_frame.index))\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    df = data_frame.reindex(np.random.permutation(data_frame.index))\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    data_set = df.values\n",
    "    return data_set\n",
    "\n",
    "#when i have delete ECG feature\n",
    "def x_data_y_data_create(data_set):\n",
    "    actual_x = data_set[:,:len(data_set[0])-2]\n",
    "    actual_y = data_set[:,len(data_set[0])-2:]\n",
    "    return actual_x,actual_y\n",
    "\n",
    "\n",
    "#full data set one label\n",
    "def x_data_y_data_create_1(data_set):\n",
    "    actual_x = data_set[:,:len(data_set[0])-1]\n",
    "    actual_y = data_set[:,len(data_set[0])-1:]\n",
    "    return actual_x,actual_y\n",
    "\n",
    "# without ECG data set one label\n",
    "def x_data_y_data_create_2(data_set):\n",
    "    actual_x = data_set[:,:len(data_set[0])-2]\n",
    "    actual_y = data_set[:,len(data_set[0])-1:]\n",
    "    return actual_x,actual_y\n",
    "\n",
    "\n",
    "#multi label data set \n",
    "def x_data_y_data_create_3(data_set):\n",
    "    actual_x = data_set[:,:len(data_set[0])-3]\n",
    "    actual_y = data_set[:,len(data_set[0])-2:]\n",
    "    return actual_x,actual_y\n",
    "\n",
    "#multi label data set without ECG\n",
    "def x_data_y_data_create_4(data_set):\n",
    "    actual_x = data_set[:,:len(data_set[0])-4]\n",
    "    actual_y = data_set[:,len(data_set[0])-2:]\n",
    "    return actual_x,actual_y\n",
    "\n",
    "def split_data(data_set):\n",
    "    # split into train and test sets\n",
    "    train_size = int(len(data_set) * 0.75)\n",
    "    test_size = len(data_set) - train_size   \n",
    "    train, test = data_set[0:train_size,:], data_set[train_size:len(data_set),:]\n",
    "\n",
    "    train_x = train[:,:len(data_set[0])-2]\n",
    "    train_y = train[:,len(data_set[0])-2:]\n",
    "\n",
    "    test_x = test[:,:len(data_set[0])-2]\n",
    "    test_y = test[:,len(data_set[0])-2:]\n",
    "\n",
    "    print(len(train), len(test))\n",
    "    \n",
    "    return train_x,train_y,test_x,test_y\n",
    "\n",
    "\n",
    "def ann_model(input_shape) : \n",
    "    model = Sequential([\n",
    "    Dense(7,kernel_initializer='uniform',input_shape=(input_shape,),activation=\"relu\"),\n",
    "    Dense(5,activation=\"relu\"),\n",
    "    Dense(2,activation=\"softmax\")\n",
    "    \n",
    "    ])\n",
    "    # print(model.summary())\n",
    "    return(model)\n",
    "\n",
    "def confussion_matrix_generator(test_y,rounded_predicted_result):\n",
    "    #rounded_predections = self.model.predict_classes(self.test_x,batch_size=10,verbose=0)\n",
    "    cm = confusion_matrix(test_y,rounded_predicted_result)\n",
    "    cm = cm\n",
    "    cm_plot_labels = ['no ', 'yes']\n",
    "    print(cm)\n",
    "    #self.plot_confusion_matrix(cm,cm_plot_labels,title='Confussion Matrix')\n",
    "    return cm\n",
    "\n",
    "\n",
    "def ROC_curve_generator(model,X_test):\n",
    "    #ROC curve\n",
    "\n",
    "    # calculate the fpr and tpr for all thresholds of the classification\n",
    "    probs = model.predict_proba(X_test)\n",
    "    preds = probs[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(Y_test, preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    # method I: plt\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def performance():\n",
    "    TN = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "    TP = cm[1][1]\n",
    "\n",
    "    n =TN+FP+FN+TP\n",
    "    print(\"numer of total test sample: \",n)\n",
    "\n",
    "    print(\"Model Score: \", score)\n",
    "\n",
    "    accuracy = (TP+TN)/n\n",
    "    print(\"accuracy: \",accuracy)\n",
    "\n",
    "    missclassafication_rate = 1-accuracy\n",
    "    print(\"missclassafication_rate: \",missclassafication_rate)\n",
    "\n",
    "    sensitivity = TP/(TP+FN)\n",
    "    print(\"Sensitivity/Recall: \",sensitivity)\n",
    "\n",
    "    spasificity = TN/(TN+FP)\n",
    "    print(\"Spacificity: \",spasificity)\n",
    "\n",
    "    precition = TP/(FP+TP)\n",
    "    print(\"Precition: \",precition)\n",
    "\n",
    "    prevalence = (FN+TP)/n\n",
    "    print(\"prevalence: \",prevalence)\n",
    "\n",
    "    PPV = TP/(TP+FP)\n",
    "    print(\"Positive Predicted Value: \",PPV)\n",
    "\n",
    "    NPV = TN/(TN+FN)\n",
    "    print(\"Negative Predicted Value: \",NPV)\n",
    "\n",
    "    beta = 0.5\n",
    "    f_score = 1/(beta*(1/precition)+(1-beta)*(1/sensitivity))\n",
    "    print(\"F Score: \",f_score)\n",
    "    \n",
    "    \n",
    "def plot_fig(i, history):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(range(1,epochs+1),history.history['val_acc'],label='validation')\n",
    "    plt.plot(range(1,epochs+1),history.history['acc'],label='training')\n",
    "    plt.legend(loc=0)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlim([1,epochs])\n",
    "#     plt.ylim([0,1])\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Model Accuracy\")\n",
    "    plt.show()\n",
    "    fig.savefig('img-'+str(i)+'-accuracy.jpg')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_frame_798 = pd.read_csv('data_798.tab', sep='\\t')\n",
    "data_frame_834 = pd.read_csv('data_834.tab', sep='\\t')\n",
    "data_frame_m = pd.read_csv('RihanSirPreprocess.tab', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_name_list = ['data_798.tab','data_834.tab','RihanSirPreprocess.tab']\n",
    "\n",
    "keras_optimizers = [\n",
    "    keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "    keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0),\n",
    "    keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0),\n",
    "    keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0),\n",
    "    keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),\n",
    "    keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0),\n",
    "    keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004),\n",
    "]\n",
    "\n",
    "\n",
    "keras_loss = [losses.mean_squared_error,\n",
    "              losses.mean_absolute_error,\n",
    "              losses.mean_absolute_percentage_error,\n",
    "              losses.mean_squared_logarithmic_error,\n",
    "              losses.squared_hinge,\n",
    "              losses.hinge,\n",
    "              losses.categorical_hinge,\n",
    "              losses.logcosh,\n",
    "              losses.categorical_crossentropy,\n",
    "              losses.sparse_categorical_crossentropy,\n",
    "              losses.binary_crossentropy,\n",
    "              losses.kullback_leibler_divergence,\n",
    "              losses.poisson,\n",
    "              losses.cosine_proximity,             \n",
    "             ]\n",
    "\n",
    "data_frame = []\n",
    "\n",
    "\n",
    "batch_size = 30\n",
    "num_classes = 2\n",
    "epochs = 5\n",
    "\n",
    "X_train =[]\n",
    "Y_train =[]\n",
    "X_test =[]\n",
    "Y_test = []\n",
    "\n",
    "input_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "834\n",
      "834\n",
      "834\n"
     ]
    }
   ],
   "source": [
    "#data_frame = read_csv_798()\n",
    "\n",
    "data_frame = read_csv_834()\n",
    "\n",
    "#data_frame = pd.read_csv('data_834.tab', sep='\\t')\n",
    "\n",
    "#data_frame = read_csv_835()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = data_frame[['50.0', '1.0', '0.0', '1.0.1', '0.0.1', '0.0.2', '0.0.3', '0.0.4',\n",
    "       '1.0.2', '0.0.5', '0.0.6', '0.0.7', '0.0.8', 'yes',\n",
    "       'no']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = create_data_set(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_set2 = np.array([])\n",
    "data_set2 = []\n",
    "for i in range(len(data_set)):\n",
    "    #print(np.insert(data_set[i],14,risk__score_calculate(data_set[i]),axis=0))\n",
    "    data_set2.append(np.insert(data_set[i],13,risk__score_calculate(data_set[i]),axis=0))\n",
    "    #np.append(data_set2, np.insert(data_set[i],13,risk__score_calculate(data_set[i]),axis=0),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = np.array(data_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42.  1.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[30.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 32.  1.  0.]\n",
      "[50.  1.  0.  1.  0.  1.  0.  1.  1.  1.  1.  0.  0. 40.  0.  1.]\n",
      "[55.  1.  1.  0.  0.  1.  0.  0.  0.  0.  1.  1.  0. 38.  1.  0.]\n",
      "[62.  2.  0.  1.  0.  1.  0.  0.  1.  0.  0.  0.  0. 33.  0.  1.]\n",
      "[30.  1.  1.  1.  0.  0.  0.  1.  0.  0.  1.  0.  0. 36.  1.  0.]\n",
      "[70.  1.  2.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 39.  1.  0.]\n",
      "[52.  1.  0.  1.  0.  0.  0.  1.  0.  0.  1.  0.  0. 34.  1.  0.]\n",
      "[60.  1.  1.  1.  1.  0.  0.  0.  1.  0.  1.  0.  0. 40.  1.  0.]\n",
      "[68.  1.  0.  1.  0.  1.  0.  1.  0.  0.  1.  1.  0. 39.  1.  0.]\n",
      "[55.  1.  1.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0. 36.  1.  0.]\n",
      "[38.  2.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. 27.  0.  1.]\n",
      "[65.  1.  1.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0. 38.  1.  0.]\n",
      "[65.  1.  0.  1.  0.  1.  0.  1.  1.  0.  1.  0.  0. 39.  0.  1.]\n",
      "[75.  1.  1.  0.  0.  1.  0.  1.  1.  0.  1.  0.  0. 40.  1.  0.]\n",
      "[45.  1.  1.  0.  0.  1.  0.  1.  1.  0.  1.  0.  0. 39.  1.  0.]\n",
      "[59.  1.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 38.  1.  0.]\n",
      "[86.  2.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  1. 35.  0.  1.]\n",
      "[32.  1.  1.  1.  0.  0.  0.  1.  1.  0.  1.  0.  0. 38.  1.  0.]\n",
      "[60.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0. 34.  1.  0.]\n",
      "[45.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0. 34.  1.  0.]\n",
      "[47.  2.  0.  1.  0.  1.  0.  1.  1.  0.  1.  0.  0. 36.  0.  1.]\n",
      "[80.  1.  0.  1.  0.  1.  0.  0.  1.  0.  0.  1.  0. 37.  1.  0.]\n",
      "[53.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 26.  0.  1.]\n",
      "[50.  2.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0. 30.  0.  1.]\n",
      "[55.  1.  2.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0. 35.  0.  1.]\n",
      "[34.  1.  0.  0.  0.  1.  1.  1.  0.  1.  1.  1.  1. 41.  0.  1.]\n",
      "[48.  1.  1.  1.  0.  0.  0.  1.  1.  0.  1.  0.  0. 39.  0.  1.]\n",
      "[64.  1.  1.  1.  0.  0.  0.  1.  1.  0.  1.  0.  0. 40.  1.  0.]\n",
      "[65.  1.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0. 33.  1.  0.]\n",
      "[65.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[29.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 26.  0.  1.]\n",
      "[50.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 28.  0.  1.]\n",
      "[65.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  1.  0. 42.  1.  0.]\n",
      "[44.  1.  1.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[43.  1.  1.  1.  1.  0.  0.  0.  1.  0.  1.  0.  0. 39.  0.  1.]\n",
      "[30.  2.  0.  1.  1.  1.  0.  1.  1.  0.  1.  0.  0. 37.  0.  1.]\n",
      "[47.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.  1.  1. 38.  1.  0.]\n",
      "[55.  1.  0.  1.  0.  1.  0.  1.  1.  0.  1.  0.  0. 39.  1.  0.]\n",
      "[43.  1.  0.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0. 36.  1.  0.]\n",
      "[75.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 29.  0.  1.]\n",
      "[50.  1.  1.  0.  0.  0.  1.  1.  0.  0.  1.  1.  1. 41.  0.  1.]\n",
      "[35.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1. 34.  0.  1.]\n",
      "[32.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0. 29.  0.  1.]\n",
      "[62.  1.  0.  1.  0.  1.  0.  0.  1.  0.  0.  0.  0. 35.  1.  0.]\n",
      "[50.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 28.  1.  0.]\n",
      "[61.  2.  0.  1.  0.  0.  1.  0.  1.  0.  1.  1.  0. 37.  0.  1.]\n",
      "[60.  2.  1.  0.  0.  0.  0.  0.  1.  0.  1.  1.  0. 36.  1.  0.]\n",
      "[60.  1.  0.  1.  0.  0.  0.  0.  1.  0.  1.  1.  0. 37.  1.  0.]\n",
      "[70.  1.  1.  1.  0.  0.  0.  0.  1.  1.  0.  1.  0. 40.  1.  0.]\n",
      "[65.  2.  0.  1.  0.  1.  0.  1.  1.  0.  0.  1.  0. 37.  0.  1.]\n",
      "[63.  1.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[35.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 27.  1.  0.]\n",
      "[55.  1.  2.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[42.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 28.  1.  0.]\n",
      "[40.  1.  0.  1.  0.  0.  0.  0.  1.  0.  1.  1.  1. 38.  0.  1.]\n",
      "[40.  2.  0.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1. 34.  0.  1.]\n",
      "[55.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 33.  1.  0.]\n",
      "[45.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 28.  0.  1.]\n",
      "[55.  1.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 38.  1.  0.]\n",
      "[60.  1.  1.  1.  0.  1.  0.  0.  0.  0.  1.  1.  0. 40.  1.  0.]\n",
      "[42.  1.  0.  1.  0.  1.  0.  1.  1.  0.  1.  0.  0. 38.  0.  1.]\n",
      "[52.  1.  1.  1.  0.  1.  0.  1.  0.  0.  1.  0.  0. 39.  0.  1.]\n",
      "[45.  2.  0.  0.  1.  1.  0.  0.  1.  0.  0.  1.  0. 34.  0.  1.]\n",
      "[60.  2.  0.  1.  1.  0.  1.  1.  1.  1.  0.  0.  0. 39.  0.  1.]\n",
      "[45.  1.  2.  1.  0.  0.  0.  0.  1.  0.  1.  1.  0. 38.  0.  1.]\n",
      "[48.  1.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[65.  1.  2.  1.  1.  0.  0.  0.  1.  0.  1.  0.  0. 39.  1.  0.]\n",
      "[75.  2.  0.  1.  0.  1.  0.  0.  1.  0.  1.  1.  0. 37.  0.  1.]\n",
      "[58.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 31.  1.  0.]\n",
      "[54.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  1.  0. 38.  1.  0.]\n",
      "[66.  1.  0.  1.  1.  1.  0.  1.  1.  0.  0.  1.  0. 41.  1.  0.]\n",
      "[47.  1.  0.  1.  1.  0.  1.  1.  1.  0.  0.  0.  0. 38.  0.  1.]\n",
      "[66.  1.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 31.  1.  0.]\n",
      "[40.  1.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0. 33.  0.  1.]\n",
      "[40.  1.  2.  1.  0.  1.  0.  0.  1.  0.  1.  1.  0. 40.  0.  1.]\n",
      "[90.  1.  2.  1.  0.  1.  1.  0.  1.  0.  1.  0.  0. 41.  1.  0.]\n",
      "[84.  1.  0.  1.  0.  0.  0.  0.  1.  1.  0.  0.  0. 35.  0.  1.]\n",
      "[30.  2.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  1. 31.  1.  0.]\n",
      "[61.  1.  0.  1.  1.  0.  0.  1.  0.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[42.  1.  1.  1.  0.  0.  0.  1.  0.  0.  1.  1.  0. 39.  1.  0.]\n",
      "[55.  1.  1.  0.  0.  0.  0.  0.  1.  0.  1.  1.  0. 38.  1.  0.]\n",
      "[44.  1.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 34.  0.  1.]\n",
      "[55.  2.  0.  0.  1.  1.  0.  0.  1.  1.  1.  0.  0. 37.  1.  0.]\n",
      "[30.  2.  0.  0.  0.  0.  0.  1.  0.  1.  0.  1.  1. 33.  0.  1.]\n",
      "[60.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1. 31.  0.  1.]\n",
      "[59.  2.  0.  1.  0.  0.  0.  0.  1.  0.  1.  1.  1. 37.  0.  1.]\n",
      "[45.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  0. 40.  1.  0.]\n",
      "[35.  1.  1.  1.  1.  0.  0.  1.  1.  0.  0.  1.  0. 40.  0.  1.]\n",
      "[45.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1. 35.  0.  1.]\n",
      "[59.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 31.  1.  0.]\n",
      "[75.  1.  2.  1.  1.  1.  0.  0.  1.  0.  1.  0.  0. 41.  0.  1.]\n",
      "[31.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0. 29.  0.  1.]\n",
      "[35.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 29.  0.  1.]\n",
      "[55.  1.  0.  1.  0.  0.  0.  0.  1.  0.  1.  1.  0. 37.  0.  1.]\n",
      "[40.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. 30.  1.  0.]\n",
      "[85.  1.  0.  1.  0.  1.  0.  0.  1.  1.  0.  0.  0. 37.  1.  0.]\n",
      "[42.  1.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  1. 34.  1.  0.]\n",
      "[55.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1. 38.  1.  0.]\n",
      "[55.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[30.  2.  0.  0.  0.  1.  0.  0.  1.  0.  1.  1.  0. 33.  0.  1.]\n",
      "[55.  1.  1.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1. 42.  1.  0.]\n",
      "[30.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1. 31.  1.  0.]\n",
      "[40.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0. 30.  0.  1.]\n",
      "[42.  1.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0. 32.  0.  1.]\n",
      "[55.  1.  1.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0. 38.  0.  1.]\n",
      "[30.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0. 29.  0.  1.]\n",
      "[50.  1.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0. 32.  1.  0.]\n",
      "[53.32490975  1.          0.          0.          0.          1.\n",
      "  0.          0.          0.          0.          1.          0.\n",
      "  0.         32.          1.          0.        ]\n",
      "[72.  1.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0. 34.  1.  0.]\n",
      "[52.  1.  0.  1.  0.  1.  0.  0.  1.  0.  0.  0.  1. 36.  1.  0.]\n",
      "[75.  2.  0.  1.  1.  1.  0.  1.  1.  0.  1.  0.  0. 39.  1.  0.]\n",
      "[40.  1.  2.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0. 34.  0.  1.]\n",
      "[55.  1.  2.  1.  1.  1.  0.  1.  1.  1.  1.  0.  0. 45.  0.  1.]\n",
      "[60.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 34.  1.  0.]\n",
      "[32.  2.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  1. 31.  1.  0.]\n",
      "[57.  1.  1.  1.  0.  0.  0.  0.  1.  1.  1.  1.  0. 42.  1.  0.]\n",
      "[38.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1. 31.  0.  1.]\n",
      "[56.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[50.  1.  0.  1.  1.  1.  0.  1.  1.  0.  1.  1.  0. 42.  1.  0.]\n",
      "[60.  1.  1.  0.  0.  1.  0.  0.  1.  0.  1.  0.  0. 38.  1.  0.]\n",
      "[53.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0. 34.  0.  1.]\n",
      "[26.  1.  1.  0.  0.  0.  0.  0.  1.  1.  0.  0.  1. 35.  0.  1.]\n",
      "[62.  1.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 36.  1.  0.]\n",
      "[45.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 33.  0.  1.]\n",
      "[65.  1.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0. 34.  1.  0.]\n",
      "[40.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. 30.  1.  0.]\n",
      "[50.  1.  1.  0.  0.  0.  0.  1.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[44.  1.  2.  0.  0.  0.  0.  1.  0.  0.  0.  1.  1. 36.  0.  1.]\n",
      "[46.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1. 35.  0.  1.]\n",
      "[60.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0. 35.  0.  1.]\n",
      "[49.  1.  1.  0.  0.  1.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[59.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 34.  1.  0.]\n",
      "[45.  1.  0.  1.  0.  1.  0.  1.  0.  0.  1.  0.  0. 36.  1.  0.]\n",
      "[42.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1. 32.  1.  0.]\n",
      "[60.  1.  1.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 40.  1.  0.]\n",
      "[58.  1.  0.  1.  0.  1.  0.  1.  1.  0.  1.  0.  0. 39.  1.  0.]\n",
      "[60.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[34.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1. 41.  1.  0.]\n",
      "[68.  1.  0.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0. 37.  0.  1.]\n",
      "[38.  1.  1.  1.  1.  1.  0.  0.  1.  0.  1.  0.  0. 40.  1.  0.]\n",
      "[77.  1.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 38.  1.  0.]\n",
      "[28.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1. 28.  0.  1.]\n",
      "[29.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1. 26.  0.  1.]\n",
      "[53.  1.  1.  1.  0.  1.  0.  0.  1.  1.  1.  0.  0. 41.  1.  0.]\n",
      "[45.  2.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 30.  1.  0.]\n",
      "[50.  2.  0.  1.  0.  1.  0.  1.  1.  0.  1.  0.  0. 36.  0.  1.]\n",
      "[41.  1.  0.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1. 40.  1.  0.]\n",
      "[45.  1.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0. 33.  0.  1.]\n",
      "[55.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0. 36.  1.  0.]\n",
      "[70.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 31.  1.  0.]\n",
      "[40.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1. 30.  0.  1.]\n",
      "[83.  1.  0.  0.  0.  1.  0.  0.  1.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[48.  2.  0.  0.  1.  0.  0.  1.  1.  0.  1.  1.  1. 38.  0.  1.]\n",
      "[60.  2.  0.  1.  0.  1.  0.  0.  1.  1.  1.  1.  0. 39.  1.  0.]\n",
      "[46.  1.  0.  1.  0.  0.  0.  1.  1.  0.  1.  0.  0. 36.  1.  0.]\n",
      "[57.  1.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0. 34.  0.  1.]\n",
      "[40.  1.  2.  1.  0.  0.  0.  1.  0.  0.  1.  0.  0. 36.  0.  1.]\n",
      "[52.  2.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0. 30.  0.  1.]\n",
      "[55.  1.  1.  0.  0.  1.  0.  0.  1.  1.  1.  1.  0. 42.  1.  0.]\n",
      "[48.  1.  2.  1.  0.  1.  1.  1.  1.  0.  1.  0.  0. 42.  1.  0.]\n",
      "[65.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 34.  1.  0.]\n",
      "[42.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0.  1.  1. 36.  1.  0.]\n",
      "[40.  1.  1.  0.  0.  0.  0.  1.  1.  0.  1.  1.  0. 39.  1.  0.]\n",
      "[45.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 28.  1.  0.]\n",
      "[35.  2.  0.  1.  0.  0.  0.  1.  1.  0.  1.  0.  0. 33.  1.  0.]\n",
      "[65.  1.  2.  1.  1.  0.  0.  0.  0.  0.  1.  1.  0. 39.  1.  0.]\n",
      "[64.  1.  1.  1.  0.  1.  0.  1.  1.  1.  1.  0.  0. 44.  1.  0.]\n",
      "[69.  1.  2.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 39.  1.  0.]\n",
      "[60.  1.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0. 33.  1.  0.]\n",
      "[40.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0. 32.  0.  1.]\n",
      "[60.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[53.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. 30.  0.  1.]\n",
      "[67.  1.  1.  1.  0.  0.  0.  0.  1.  0.  1.  1.  0. 40.  1.  0.]\n",
      "[30.  2.  0.  1.  1.  1.  0.  1.  1.  0.  1.  0.  0. 37.  0.  1.]\n",
      "[70.  1.  0.  1.  0.  1.  0.  0.  0.  0.  1.  1.  0. 37.  0.  1.]\n",
      "[60.  2.  0.  1.  0.  1.  0.  1.  0.  0.  0.  0.  0. 33.  0.  1.]\n",
      "[60.  2.  1.  1.  0.  1.  0.  1.  1.  0.  1.  1.  0. 42.  1.  0.]\n",
      "[57.  1.  1.  1.  0.  1.  0.  0.  1.  0.  0.  1.  0. 40.  1.  0.]\n",
      "[71.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0.  0.  0. 35.  0.  1.]\n",
      "[75.  1.  2.  1.  0.  0.  0.  0.  1.  1.  1.  0.  0. 39.  1.  0.]\n",
      "[40.  2.  0.  0.  0.  1.  0.  1.  0.  0.  1.  0.  0. 32.  1.  0.]\n",
      "[40.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  0.  1. 41.  1.  0.]\n",
      "[40.  2.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  1. 32.  1.  0.]\n",
      "[75.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 33.  1.  0.]\n",
      "[49.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. 33.  0.  1.]\n",
      "[60.  2.  0.  1.  0.  1.  0.  0.  1.  0.  0.  0.  0. 33.  1.  0.]\n",
      "[57.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0. 34.  1.  0.]\n",
      "[43.  1.  0.  1.  0.  1.  0.  0.  1.  0.  0.  0.  0. 34.  0.  1.]\n",
      "[47.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1. 32.  1.  0.]\n",
      "[64.  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0. 33.  0.  1.]\n",
      "[48.  1.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1. 35.  0.  1.]\n",
      "[50.  2.  0.  1.  0.  0.  0.  0.  1.  1.  1.  1.  0. 36.  1.  0.]\n",
      "[38.  1.  2.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[45.  2.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0. 32.  1.  0.]\n",
      "[58.  1.  0.  1.  0.  0.  0.  0.  1.  0.  1.  1.  0. 37.  1.  0.]\n",
      "[50.  1.  1.  0.  0.  1.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[57.  1.  1.  1.  1.  0.  1.  0.  1.  1.  1.  0.  0. 44.  0.  1.]\n",
      "[45.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 31.  0.  1.]\n",
      "[44.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1. 32.  0.  1.]\n",
      "[55.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  0.  0. 46.  1.  0.]\n",
      "[75.  2.  0.  1.  0.  0.  0.  0.  1.  0.  1.  1.  0. 35.  0.  1.]\n",
      "[50.  1.  1.  1.  0.  0.  0.  0.  1.  0.  1.  1.  0. 39.  1.  0.]\n",
      "[55.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0. 36.  1.  0.]\n",
      "[72.  1.  2.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 37.  0.  1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63.  1.  0.  0.  0.  1.  1.  0.  1.  0.  0.  1.  0. 37.  1.  0.]\n",
      "[58.  1.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0. 35.  1.  0.]\n",
      "[75.  2.  0.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1. 45.  1.  0.]\n",
      "[59.  1.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[80.  1.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0. 33.  1.  0.]\n",
      "[32.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 30.  0.  1.]\n",
      "[48.  2.  0.  1.  0.  0.  0.  1.  1.  0.  1.  0.  0. 34.  0.  1.]\n",
      "[42.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 33.  1.  0.]\n",
      "[32.  2.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  1. 31.  0.  1.]\n",
      "[56.  2.  0.  1.  0.  1.  0.  0.  1.  0.  0.  0.  0. 33.  1.  0.]\n",
      "[75.  1.  1.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0. 38.  0.  1.]\n",
      "[45.  1.  1.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0. 35.  1.  0.]\n",
      "[38.  1.  1.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0. 34.  1.  0.]\n",
      "[55.  1.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0. 33.  1.  0.]\n",
      "[52.  1.  0.  1.  0.  1.  0.  0.  1.  0.  0.  1.  1. 38.  0.  1.]\n",
      "[67.  1.  1.  1.  0.  0.  0.  0.  0.  0.  1.  1.  0. 38.  0.  1.]\n",
      "[75.  1.  0.  1.  1.  1.  0.  0.  1.  1.  1.  1.  0. 43.  1.  0.]\n",
      "[30.  1.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0. 34.  0.  1.]\n",
      "[37.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. 32.  0.  1.]\n",
      "[42.  1.  1.  1.  1.  0.  1.  1.  1.  1.  0.  0.  0. 43.  1.  0.]\n",
      "[54.  1.  1.  1.  0.  0.  0.  0.  1.  1.  0.  1.  0. 39.  1.  0.]\n",
      "[61.  1.  0.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0. 37.  0.  1.]\n",
      "[51.  2.  0.  1.  0.  1.  0.  0.  1.  0.  0.  1.  0. 34.  1.  0.]\n",
      "[65.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0. 31.  0.  1.]\n",
      "[59.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0. 31.  0.  1.]\n",
      "[48.  2.  0.  1.  0.  1.  0.  1.  1.  0.  1.  1.  1. 40.  0.  1.]\n",
      "[52.  1.  1.  1.  0.  0.  0.  0.  1.  0.  0.  1.  0. 37.  1.  0.]\n",
      "[37.  1.  1.  0.  0.  0.  0.  1.  1.  0.  0.  1.  0. 36.  1.  0.]\n",
      "[65.  1.  2.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[40.  2.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0. 28.  0.  1.]\n",
      "[60.  1.  1.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0. 38.  1.  0.]\n",
      "[60.  1.  1.  1.  0.  1.  0.  0.  1.  0.  1.  1.  0. 42.  1.  0.]\n",
      "[45.  1.  1.  0.  0.  0.  0.  0.  1.  0.  1.  1.  0. 37.  1.  0.]\n",
      "[40.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  1. 34.  1.  0.]\n",
      "[43.  2.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 34.  1.  0.]\n",
      "[46.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 31.  0.  1.]\n",
      "[55.  1.  1.  0.  1.  1.  0.  0.  1.  0.  1.  0.  0. 40.  1.  0.]\n",
      "[25.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1. 30.  0.  1.]\n",
      "[60.  1.  0.  1.  0.  0.  0.  1.  1.  0.  1.  1.  1. 41.  1.  0.]\n",
      "[25.  1.  0.  1.  0.  0.  0.  1.  0.  0.  1.  0.  1. 34.  0.  1.]\n",
      "[56.  2.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[60.  1.  0.  1.  0.  1.  0.  0.  1.  0.  0.  1.  1. 39.  0.  1.]\n",
      "[53.  1.  2.  1.  0.  0.  0.  1.  1.  0.  1.  0.  0. 38.  0.  1.]\n",
      "[51.  1.  0.  1.  1.  1.  0.  1.  1.  1.  1.  0.  1. 44.  0.  1.]\n",
      "[75.  1.  2.  1.  1.  0.  0.  1.  1.  0.  0.  1.  0. 41.  0.  1.]\n",
      "[39.  2.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0. 31.  0.  1.]\n",
      "[36.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 27.  0.  1.]\n",
      "[50.  1.  2.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0. 34.  1.  0.]\n",
      "[48.  1.  0.  0.  0.  1.  0.  0.  1.  0.  0.  1.  0. 34.  0.  1.]\n",
      "[65.  1.  0.  1.  0.  0.  1.  0.  1.  1.  0.  0.  0. 37.  1.  0.]\n",
      "[90.  1.  1.  1.  0.  0.  0.  1.  1.  0.  0.  0.  1. 40.  1.  0.]\n",
      "[56.  2.  0.  1.  0.  1.  1.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[62.  1.  2.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[46.  1.  0.  1.  1.  1.  0.  0.  1.  1.  1.  1.  0. 42.  1.  0.]\n",
      "[64.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 29.  1.  0.]\n",
      "[45.  1.  1.  1.  1.  0.  0.  0.  1.  0.  1.  0.  0. 39.  1.  0.]\n",
      "[55.  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.  0.  0. 44.  1.  0.]\n",
      "[52.  1.  0.  1.  1.  1.  1.  1.  1.  0.  1.  1.  0. 44.  1.  0.]\n",
      "[65.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 37.  0.  1.]\n",
      "[54.  1.  1.  1.  0.  1.  0.  1.  1.  0.  1.  0.  0. 41.  1.  0.]\n",
      "[65.  1.  0.  1.  0.  0.  0.  1.  1.  0.  1.  1.  0. 39.  0.  1.]\n",
      "[62.  1.  2.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[75.  2.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0. 31.  1.  0.]\n",
      "[50.  1.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0. 43.  1.  0.]\n",
      "[60.  1.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[55.  1.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0. 33.  1.  0.]\n",
      "[40.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 33.  1.  0.]\n",
      "[78.  2.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[60.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[50.  2.  0.  1.  0.  1.  0.  0.  0.  0.  1.  1.  1. 36.  1.  0.]\n",
      "[45.  1.  1.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[55.  2.  0.  1.  0.  0.  0.  0.  0.  0.  1.  1.  0. 33.  1.  0.]\n",
      "[56.  1.  2.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0. 35.  1.  0.]\n",
      "[64.  1.  1.  0.  0.  0.  0.  0.  1.  0.  1.  1.  0. 38.  0.  1.]\n",
      "[65.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[52.  1.  1.  0.  0.  0.  0.  1.  0.  1.  1.  0.  0. 37.  1.  0.]\n",
      "[25.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 26.  1.  0.]\n",
      "[55.  1.  0.  0.  0.  0.  0.  1.  1.  1.  1.  0.  0. 37.  1.  0.]\n",
      "[51.  1.  2.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0. 34.  0.  1.]\n",
      "[69.  1.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0. 44.  0.  1.]\n",
      "[55.  1.  1.  0.  0.  0.  0.  1.  1.  0.  1.  1.  0. 40.  1.  0.]\n",
      "[45.  1.  2.  0.  0.  0.  0.  1.  1.  1.  1.  0.  0. 38.  1.  0.]\n",
      "[72.  1.  2.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0. 39.  1.  0.]\n",
      "[45.  1.  1.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[85.  1.  2.  1.  0.  1.  0.  0.  1.  0.  0.  0.  0. 37.  0.  1.]\n",
      "[75.  2.  0.  0.  0.  0.  0.  1.  1.  0.  0.  1.  0. 33.  1.  0.]\n",
      "[56.  1.  0.  1.  0.  1.  0.  1.  1.  0.  1.  0.  0. 39.  1.  0.]\n",
      "[50.  2.  0.  0.  0.  1.  0.  0.  1.  0.  1.  0.  0. 32.  1.  0.]\n",
      "[40.  2.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0. 30.  0.  1.]\n",
      "[68.  1.  0.  1.  0.  1.  0.  1.  1.  0.  1.  1.  0. 41.  1.  0.]\n",
      "[55.  1.  1.  1.  0.  0.  0.  0.  0.  0.  1.  1.  0. 38.  1.  0.]\n",
      "[53.32490975  2.          0.          1.          0.          1.\n",
      "  0.          0.          0.          0.          1.          1.\n",
      "  1.         36.          1.          0.        ]\n",
      "[48.  2.  0.  0.  0.  1.  1.  0.  0.  0.  1.  1.  1. 36.  0.  1.]\n",
      "[28.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 26.  0.  1.]\n",
      "[70.  1.  2.  1.  1.  0.  0.  1.  1.  1.  1.  1.  1. 47.  0.  1.]\n",
      "[40.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0. 30.  1.  0.]\n",
      "[63.  1.  0.  1.  0.  1.  0.  0.  1.  0.  0.  0.  0. 35.  1.  0.]\n",
      "[57.  2.  0.  1.  1.  1.  1.  0.  1.  1.  1.  1.  0. 43.  1.  0.]\n",
      "[45.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 28.  0.  1.]\n",
      "[94.  1.  0.  1.  0.  1.  1.  0.  1.  0.  1.  1.  0. 41.  1.  0.]\n",
      "[28.  1.  0.  0.  0.  0.  1.  0.  0.  0.  1.  1.  1. 34.  0.  1.]\n",
      "[60.  1.  0.  1.  0.  1.  0.  0.  1.  0.  0.  0.  0. 35.  1.  0.]\n",
      "[73.  2.  0.  1.  1.  0.  0.  1.  1.  0.  1.  1.  0. 39.  1.  0.]\n",
      "[57.  2.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0. 33.  0.  1.]\n",
      "[40.  1.  1.  1.  0.  0.  0.  1.  1.  0.  1.  0.  0. 39.  1.  0.]\n",
      "[67.  1.  1.  1.  0.  0.  0.  1.  1.  1.  0.  1.  1. 44.  1.  0.]\n",
      "[55.  2.  0.  1.  1.  1.  0.  0.  1.  1.  1.  1.  0. 41.  0.  1.]\n",
      "[45.  1.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1.  1. 38.  0.  1.]\n",
      "[67.  1.  0.  1.  0.  1.  0.  0.  1.  1.  1.  1.  0. 41.  1.  0.]\n",
      "[67.  2.  0.  1.  0.  1.  0.  0.  1.  0.  0.  0.  0. 33.  0.  1.]\n",
      "[50.  1.  0.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0. 34.  1.  0.]\n",
      "[42.  1.  1.  0.  0.  0.  0.  1.  0.  0.  1.  0.  1. 37.  0.  1.]\n",
      "[57.  2.  0.  1.  0.  1.  1.  0.  1.  0.  0.  0.  0. 35.  0.  1.]\n",
      "[52.  1.  2.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0. 34.  0.  1.]\n",
      "[63.  1.  2.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 39.  0.  1.]\n",
      "[52.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0. 33.  0.  1.]\n",
      "[63.  1.  0.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0. 37.  0.  1.]\n",
      "[50.  2.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0. 30.  0.  1.]\n",
      "[45.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 32.  0.  1.]\n",
      "[60.  1.  0.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1. 39.  1.  0.]\n",
      "[80.  1.  1.  1.  0.  1.  0.  0.  1.  0.  0.  0.  0. 38.  1.  0.]\n",
      "[45.  2.  0.  1.  0.  0.  1.  1.  1.  1.  1.  1.  0. 40.  0.  1.]\n",
      "[62.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  0. 48.  1.  0.]\n",
      "[55.  1.  1.  0.  0.  0.  0.  0.  1.  0.  1.  1.  1. 40.  1.  0.]\n",
      "[44.  1.  1.  1.  0.  1.  0.  0.  1.  1.  1.  0.  0. 41.  1.  0.]\n",
      "[66.  2.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  1. 35.  0.  1.]\n",
      "[63.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[47.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. 30.  1.  0.]\n",
      "[45.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 31.  0.  1.]\n",
      "[48.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 36.  0.  1.]\n",
      "[26.  2.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  1. 30.  0.  1.]\n",
      "[55.  2.  0.  1.  1.  0.  0.  0.  1.  0.  0.  0.  0. 33.  1.  0.]\n",
      "[40.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1. 32.  0.  1.]\n",
      "[43.  2.  1.  0.  0.  0.  0.  1.  1.  0.  0.  0.  1. 35.  1.  0.]\n",
      "[70.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[67.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 37.  0.  1.]\n",
      "[40.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1. 30.  0.  1.]\n",
      "[80.  1.  2.  1.  0.  0.  1.  0.  0.  0.  1.  0.  0. 37.  0.  1.]\n",
      "[50.  1.  1.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 39.  1.  0.]\n",
      "[44.  1.  0.  0.  0.  0.  0.  1.  1.  0.  1.  1.  1. 38.  0.  1.]\n",
      "[44.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1. 32.  0.  1.]\n",
      "[50.  1.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.  1. 36.  0.  1.]\n",
      "[26.  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1. 34.  1.  0.]\n",
      "[65.  1.  1.  1.  0.  0.  0.  1.  1.  1.  1.  0.  0. 42.  1.  0.]\n",
      "[62.  1.  1.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 40.  0.  1.]\n",
      "[35.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 30.  0.  1.]\n",
      "[55.  1.  1.  1.  0.  1.  0.  0.  0.  1.  1.  1.  1. 44.  1.  0.]\n",
      "[52.  2.  0.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0. 32.  1.  0.]\n",
      "[69.  1.  0.  1.  1.  1.  1.  0.  1.  1.  1.  1.  0. 45.  1.  0.]\n",
      "[48.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0. 35.  1.  0.]\n",
      "[76.  2.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  1. 33.  0.  1.]\n",
      "[60.  1.  2.  1.  0.  1.  0.  1.  0.  0.  1.  1.  0. 41.  1.  0.]\n",
      "[60.  1.  2.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0. 33.  1.  0.]\n",
      "[61.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  0. 41.  1.  0.]\n",
      "[60.  1.  0.  0.  0.  1.  0.  1.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[37.  1.  0.  0.  0.  0.  0.  1.  1.  0.  1.  0.  0. 33.  1.  0.]\n",
      "[67.  1.  2.  1.  0.  0.  1.  0.  1.  0.  1.  1.  0. 41.  1.  0.]\n",
      "[56.  2.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 31.  0.  1.]\n",
      "[50.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0. 32.  1.  0.]\n",
      "[55.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0. 35.  0.  1.]\n",
      "[55.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0. 31.  1.  0.]\n",
      "[55.  1.  1.  1.  0.  0.  0.  0.  1.  0.  1.  1.  0. 40.  1.  0.]\n",
      "[80.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 34.  1.  0.]\n",
      "[65.  2.  0.  1.  0.  0.  0.  1.  1.  0.  0.  1.  0. 35.  0.  1.]\n",
      "[35.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 25.  0.  1.]\n",
      "[72.  1.  1.  1.  0.  0.  0.  0.  1.  1.  0.  0.  0. 38.  1.  0.]\n",
      "[55.  1.  0.  0.  0.  1.  0.  0.  1.  0.  1.  1.  0. 37.  1.  0.]\n",
      "[52.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0. 35.  1.  0.]\n",
      "[46.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  1. 36.  1.  0.]\n",
      "[47.  2.  0.  1.  1.  0.  1.  1.  1.  0.  1.  0.  1. 40.  1.  0.]\n",
      "[60.  2.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0. 31.  0.  1.]\n",
      "[70.  1.  1.  1.  0.  1.  0.  0.  1.  0.  0.  1.  0. 40.  0.  1.]\n",
      "[45.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. 30.  0.  1.]\n",
      "[48.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 31.  0.  1.]\n",
      "[56.  1.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0. 33.  1.  0.]\n",
      "[49.  1.  1.  1.  0.  0.  0.  1.  1.  0.  1.  0.  0. 39.  1.  0.]\n",
      "[40.  1.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  1. 34.  0.  1.]\n",
      "[57.  1.  1.  1.  0.  0.  0.  1.  1.  0.  1.  1.  0. 42.  1.  0.]\n",
      "[32.  1.  0.  0.  0.  0.  0.  1.  0.  1.  1.  0.  1. 35.  0.  1.]\n",
      "[60.  2.  0.  1.  0.  1.  0.  1.  1.  1.  1.  0.  0. 39.  1.  0.]\n",
      "[65.  1.  2.  0.  0.  1.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[43.  2.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0. 30.  0.  1.]\n",
      "[21.  2.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  1. 32.  0.  1.]\n",
      "[47.  1.  2.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 36.  1.  0.]\n",
      "[70.  1.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0. 33.  1.  0.]\n",
      "[33.  2.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 31.  1.  0.]\n",
      "[52.  2.  0.  1.  0.  0.  0.  0.  0.  1.  1.  0.  0. 32.  1.  0.]\n",
      "[65.  1.  2.  1.  0.  1.  0.  0.  1.  0.  1.  1.  1. 43.  1.  0.]\n",
      "[62.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 29.  0.  1.]\n",
      "[55.  1.  1.  0.  0.  0.  0.  0.  1.  1.  0.  0.  1. 38.  0.  1.]\n",
      "[33.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. 32.  0.  1.]\n",
      "[55.  1.  1.  0.  0.  1.  0.  0.  1.  0.  1.  0.  0. 38.  1.  0.]\n",
      "[65.  2.  0.  1.  0.  1.  0.  0.  1.  0.  0.  1.  1. 37.  1.  0.]\n",
      "[76.  1.  0.  1.  0.  0.  0.  0.  1.  1.  1.  1.  0. 39.  1.  0.]\n",
      "[75.  1.  2.  1.  0.  1.  0.  0.  1.  0.  0.  0.  1. 39.  0.  1.]\n",
      "[55.  1.  2.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 33.  1.  0.]\n",
      "[30.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1. 27.  0.  1.]\n",
      "[85.  2.  0.  1.  0.  1.  0.  0.  0.  0.  1.  1.  0. 35.  0.  1.]\n",
      "[62.  2.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[55.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0. 31.  0.  1.]\n",
      "[48.  1.  0.  1.  0.  1.  0.  0.  1.  0.  0.  0.  0. 34.  1.  0.]\n",
      "[38.  1.  0.  0.  0.  1.  0.  0.  1.  1.  0.  1.  1. 37.  1.  0.]\n",
      "[70.  1.  1.  0.  0.  0.  0.  0.  1.  0.  1.  1.  0. 38.  1.  0.]\n",
      "[41.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 28.  0.  1.]\n",
      "[60.  1.  1.  1.  0.  1.  0.  0.  1.  0.  0.  1.  0. 40.  1.  0.]\n",
      "[55.  1.  0.  1.  0.  0.  0.  0.  1.  0.  1.  1.  0. 37.  1.  0.]\n",
      "[50.  2.  0.  1.  1.  1.  1.  0.  1.  1.  1.  1.  0. 42.  1.  0.]\n",
      "[60.  1.  1.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 40.  1.  0.]\n",
      "[75.  1.  2.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 35.  1.  0.]\n",
      "[80.  1.  0.  1.  0.  0.  1.  1.  1.  1.  0.  0.  0. 39.  0.  1.]\n",
      "[30.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  1. 33.  1.  0.]\n",
      "[65.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0. 34.  0.  1.]\n",
      "[38.  1.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.  0. 33.  1.  0.]\n",
      "[45.  2.  0.  1.  1.  0.  0.  0.  1.  0.  1.  1.  0. 36.  0.  1.]\n",
      "[42.  1.  1.  0.  0.  1.  0.  1.  1.  0.  1.  0.  0. 39.  0.  1.]\n",
      "[48.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 28.  0.  1.]\n",
      "[59.  1.  2.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 39.  1.  0.]\n",
      "[72.  1.  0.  1.  0.  0.  1.  0.  1.  1.  0.  0.  0. 37.  1.  0.]\n",
      "[55.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0. 33.  1.  0.]\n",
      "[45.  1.  1.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[48.  1.  0.  0.  0.  1.  0.  1.  0.  0.  1.  0.  1. 36.  0.  1.]\n",
      "[45.  2.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 32.  1.  0.]\n",
      "[59.  1.  2.  0.  0.  0.  0.  0.  1.  0.  1.  1.  1. 39.  1.  0.]\n",
      "[65.  1.  2.  1.  0.  1.  0.  0.  1.  1.  1.  1.  0. 43.  0.  1.]\n",
      "[31.  1.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0. 32.  1.  0.]\n",
      "[47.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. 30.  1.  0.]\n",
      "[68.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[49.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  1. 36.  0.  1.]\n",
      "[46.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0. 30.  0.  1.]\n",
      "[64.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1. 33.  0.  1.]\n",
      "[60.  1.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[58.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0. 31.  1.  0.]\n",
      "[65.  1.  0.  1.  0.  0.  0.  0.  1.  0.  1.  1.  0. 37.  1.  0.]\n",
      "[40.  2.  0.  0.  0.  1.  0.  1.  1.  0.  0.  0.  1. 34.  0.  1.]\n",
      "[50.  2.  0.  1.  0.  1.  0.  0.  1.  0.  0.  0.  1. 34.  0.  1.]\n",
      "[58.  1.  0.  1.  0.  0.  0.  1.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[42.  2.  0.  0.  0.  1.  0.  1.  1.  0.  0.  1.  1. 36.  0.  1.]\n",
      "[85.  2.  0.  1.  1.  1.  1.  0.  1.  0.  1.  0.  1. 41.  0.  1.]\n",
      "[58.  2.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 36.  1.  0.]\n",
      "[48.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1. 30.  0.  1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50.  1.  1.  1.  1.  0.  1.  0.  1.  0.  1.  1.  1. 45.  1.  0.]\n",
      "[75.  1.  0.  0.  0.  1.  0.  0.  1.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[53.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  1. 36.  1.  0.]\n",
      "[60.  2.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  1. 33.  1.  0.]\n",
      "[55.  1.  1.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0. 36.  1.  0.]\n",
      "[62.  1.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 36.  1.  0.]\n",
      "[55.  1.  1.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 40.  1.  0.]\n",
      "[52.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  1.  1. 40.  1.  0.]\n",
      "[47.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0. 32.  0.  1.]\n",
      "[55.  1.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 38.  1.  0.]\n",
      "[48.  1.  0.  1.  0.  1.  0.  1.  1.  0.  1.  1.  0. 40.  0.  1.]\n",
      "[91.  2.  0.  1.  0.  1.  0.  0.  1.  0.  1.  1.  0. 37.  1.  0.]\n",
      "[67.  1.  2.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 39.  1.  0.]\n",
      "[54.  1.  1.  1.  1.  0.  0.  0.  1.  0.  1.  0.  0. 39.  1.  0.]\n",
      "[45.  1.  2.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 36.  0.  1.]\n",
      "[46.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0. 30.  0.  1.]\n",
      "[55.  2.  0.  1.  0.  1.  0.  0.  0.  0.  1.  1.  0. 35.  1.  0.]\n",
      "[42.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0. 30.  0.  1.]\n",
      "[58.  1.  2.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[70.  1.  0.  0.  0.  1.  0.  0.  1.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[45.  1.  1.  0.  0.  1.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[42.  1.  1.  1.  0.  1.  0.  1.  1.  0.  1.  0.  0. 41.  1.  0.]\n",
      "[36.  1.  1.  0.  0.  1.  0.  0.  1.  0.  1.  0.  0. 36.  1.  0.]\n",
      "[26.  1.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1. 33.  0.  1.]\n",
      "[45.  1.  0.  1.  0.  0.  0.  0.  1.  1.  1.  0.  0. 36.  0.  1.]\n",
      "[50.  1.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0. 33.  0.  1.]\n",
      "[45.  2.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0. 30.  0.  1.]\n",
      "[56.  1.  0.  1.  1.  0.  1.  0.  1.  1.  1.  0.  0. 41.  1.  0.]\n",
      "[85.  2.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0. 31.  0.  1.]\n",
      "[50.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 30.  1.  0.]\n",
      "[54.  1.  0.  1.  0.  0.  0.  1.  1.  1.  1.  0.  0. 38.  0.  1.]\n",
      "[70.  1.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0. 36.  1.  0.]\n",
      "[26.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0. 30.  1.  0.]\n",
      "[50.  2.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.  1. 34.  0.  1.]\n",
      "[52.  1.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0. 32.  0.  1.]\n",
      "[65.  2.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 33.  1.  0.]\n",
      "[50.  1.  1.  0.  0.  0.  0.  1.  1.  1.  0.  1.  0. 39.  1.  0.]\n",
      "[48.  1.  2.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0. 36.  0.  1.]\n",
      "[45.  2.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 30.  1.  0.]\n",
      "[49.  1.  1.  1.  0.  1.  0.  1.  1.  0.  1.  0.  0. 41.  1.  0.]\n",
      "[35.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1. 31.  0.  1.]\n",
      "[30.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. 29.  0.  1.]\n",
      "[46.  2.  0.  1.  0.  0.  0.  0.  1.  0.  0.  1.  1. 34.  0.  1.]\n",
      "[70.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0. 34.  1.  0.]\n",
      "[51.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  1. 34.  0.  1.]\n",
      "[55.  1.  1.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0. 36.  1.  0.]\n",
      "[70.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 29.  0.  1.]\n",
      "[24.  1.  1.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0. 33.  1.  0.]\n",
      "[45.  2.  0.  0.  1.  1.  0.  0.  1.  0.  1.  0.  1. 36.  1.  0.]\n",
      "[60.  1.  1.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 40.  1.  0.]\n",
      "[46.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. 30.  0.  1.]\n",
      "[60.  1.  1.  1.  0.  1.  0.  1.  0.  0.  0.  0.  0. 38.  1.  0.]\n",
      "[15.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0. 28.  0.  1.]\n",
      "[50.  1.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 30.  1.  0.]\n",
      "[55.  1.  2.  1.  0.  0.  0.  0.  1.  0.  0.  1.  0. 37.  1.  0.]\n",
      "[40.  1.  2.  1.  0.  0.  0.  1.  1.  0.  0.  0.  1. 38.  1.  0.]\n",
      "[33.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1. 29.  0.  1.]\n",
      "[69.  1.  0.  1.  1.  1.  0.  0.  1.  1.  1.  1.  0. 43.  1.  0.]\n",
      "[65.  1.  0.  0.  0.  0.  0.  1.  1.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[45.  1.  1.  0.  1.  1.  1.  0.  1.  1.  1.  1.  0. 45.  1.  0.]\n",
      "[46.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0.  0.  0. 34.  1.  0.]\n",
      "[45.  2.  0.  1.  0.  0.  0.  1.  1.  1.  0.  1.  1. 38.  0.  1.]\n",
      "[55.  1.  1.  0.  0.  0.  1.  0.  0.  1.  1.  0.  1. 40.  1.  0.]\n",
      "[54.  1.  2.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 34.  1.  0.]\n",
      "[38.  2.  0.  1.  1.  0.  0.  0.  1.  0.  0.  0.  0. 31.  0.  1.]\n",
      "[59.  1.  1.  0.  0.  1.  1.  1.  1.  1.  0.  1.  0. 44.  0.  1.]\n",
      "[61.  1.  1.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0. 38.  1.  0.]\n",
      "[60.  2.  0.  1.  0.  0.  0.  0.  1.  0.  0.  1.  0. 33.  1.  0.]\n",
      "[47.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0. 32.  0.  1.]\n",
      "[28.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. 28.  0.  1.]\n",
      "[80.  1.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 38.  1.  0.]\n",
      "[53.32490975  1.          1.          0.          0.          0.\n",
      "  0.          1.          1.          1.          0.          0.\n",
      "  0.         37.          0.          1.        ]\n",
      "[26.  1.  0.  1.  0.  0.  0.  1.  1.  0.  1.  1.  0. 36.  1.  0.]\n",
      "[40.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 31.  0.  1.]\n",
      "[46.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 28.  0.  1.]\n",
      "[45.  1.  1.  1.  0.  0.  0.  0.  1.  0.  1.  1.  0. 39.  1.  0.]\n",
      "[64.  1.  0.  1.  0.  1.  0.  0.  1.  1.  1.  0.  0. 39.  1.  0.]\n",
      "[41.  1.  1.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[65.  2.  0.  1.  1.  0.  0.  0.  1.  0.  1.  1.  0. 37.  1.  0.]\n",
      "[50.  1.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0. 32.  1.  0.]\n",
      "[65.  1.  1.  1.  0.  0.  0.  0.  1.  1.  0.  0.  0. 38.  1.  0.]\n",
      "[75.  1.  0.  1.  0.  1.  0.  0.  1.  0.  0.  0.  0. 35.  0.  1.]\n",
      "[70.  1.  0.  1.  0.  0.  0.  1.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[42.  2.  0.  0.  1.  0.  0.  0.  1.  1.  1.  1.  0. 36.  0.  1.]\n",
      "[57.  2.  0.  1.  0.  1.  0.  0.  1.  0.  0.  0.  0. 33.  1.  0.]\n",
      "[53.  2.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0. 30.  0.  1.]\n",
      "[37.  1.  1.  0.  0.  1.  0.  0.  1.  0.  1.  0.  0. 36.  1.  0.]\n",
      "[85.  2.  0.  1.  0.  1.  0.  1.  1.  1.  1.  1.  0. 41.  1.  0.]\n",
      "[42.  1.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  1. 34.  0.  1.]\n",
      "[62.  2.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 31.  1.  0.]\n",
      "[70.  2.  0.  1.  0.  0.  0.  0.  1.  0.  0.  1.  0. 33.  1.  0.]\n",
      "[50.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 28.  1.  0.]\n",
      "[60.  2.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 31.  0.  1.]\n",
      "[62.  2.  0.  1.  0.  1.  0.  0.  1.  1.  1.  1.  0. 39.  1.  0.]\n",
      "[40.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. 33.  1.  0.]\n",
      "[60.  1.  2.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[68.  1.  2.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0. 35.  0.  1.]\n",
      "[70.  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.  0.  0. 44.  1.  0.]\n",
      "[75.  1.  0.  1.  0.  0.  0.  1.  1.  0.  1.  1.  0. 39.  1.  0.]\n",
      "[32.  1.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 36.  0.  1.]\n",
      "[65.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 33.  1.  0.]\n",
      "[41.  2.  0.  0.  0.  0.  0.  1.  1.  0.  1.  1.  0. 34.  1.  0.]\n",
      "[60.  1.  2.  1.  0.  1.  0.  0.  1.  0.  1.  1.  0. 41.  1.  0.]\n",
      "[45.  1.  1.  0.  0.  0.  0.  0.  1.  0.  1.  0.  1. 37.  0.  1.]\n",
      "[60.  2.  0.  1.  0.  1.  0.  0.  1.  0.  0.  1.  0. 35.  1.  0.]\n",
      "[42.  1.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0. 32.  0.  1.]\n",
      "[45.  1.  0.  1.  0.  0.  1.  1.  0.  0.  0.  0.  1. 36.  1.  0.]\n",
      "[46.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 31.  0.  1.]\n",
      "[57.  1.  0.  1.  0.  0.  0.  1.  1.  0.  0.  1.  1. 39.  0.  1.]\n",
      "[55.  1.  1.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0. 36.  1.  0.]\n",
      "[50.  1.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.  1. 36.  1.  0.]\n",
      "[51.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 28.  0.  1.]\n",
      "[50.  1.  1.  1.  0.  1.  0.  1.  1.  0.  1.  0.  0. 41.  1.  0.]\n",
      "[65.  1.  0.  1.  0.  1.  0.  0.  1.  0.  0.  0.  0. 35.  1.  0.]\n",
      "[69.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 33.  0.  1.]\n",
      "[42.  1.  2.  1.  0.  1.  0.  0.  1.  1.  0.  0.  0. 38.  1.  0.]\n",
      "[62.  1.  2.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[54.  2.  0.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0. 34.  1.  0.]\n",
      "[65.  2.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 35.  0.  1.]\n",
      "[56.  1.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0. 35.  1.  0.]\n",
      "[55.  1.  0.  1.  0.  1.  0.  0.  1.  0.  0.  0.  0. 35.  1.  0.]\n",
      "[46.  1.  1.  1.  0.  0.  0.  1.  1.  0.  1.  1.  1. 43.  0.  1.]\n",
      "[26.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 30.  1.  0.]\n",
      "[50.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 28.  0.  1.]\n",
      "[48.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  1. 34.  0.  1.]\n",
      "[35.  1.  2.  0.  0.  0.  0.  1.  1.  0.  1.  1.  1. 39.  1.  0.]\n",
      "[28.  1.  1.  0.  0.  0.  0.  1.  1.  0.  1.  1.  0. 37.  0.  1.]\n",
      "[44.  1.  0.  1.  1.  0.  0.  0.  1.  0.  1.  0.  1. 38.  1.  0.]\n",
      "[50.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 26.  0.  1.]\n",
      "[38.  2.  0.  0.  0.  0.  0.  1.  1.  0.  1.  1.  0. 33.  0.  1.]\n",
      "[60.  1.  1.  1.  1.  0.  0.  1.  1.  1.  1.  1.  0. 46.  1.  0.]\n",
      "[69.  2.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0. 31.  1.  0.]\n",
      "[66.  2.  0.  1.  0.  1.  0.  1.  1.  0.  0.  1.  0. 37.  0.  1.]\n",
      "[50.  1.  2.  1.  1.  1.  0.  0.  0.  0.  1.  0.  0. 38.  1.  0.]\n",
      "[70.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1. 33.  0.  1.]\n",
      "[53.  1.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[82.  2.  0.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0. 35.  1.  0.]\n",
      "[65.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. 31.  0.  1.]\n",
      "[47.  2.  0.  1.  0.  1.  0.  1.  1.  0.  1.  1.  0. 38.  1.  0.]\n",
      "[47.  1.  2.  1.  0.  1.  0.  0.  1.  0.  0.  0.  0. 36.  1.  0.]\n",
      "[39.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 32.  0.  1.]\n",
      "[94.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  1.  0. 39.  1.  0.]\n",
      "[53.  1.  1.  1.  0.  1.  0.  1.  1.  1.  0.  0.  0. 41.  0.  1.]\n",
      "[50.  1.  2.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 38.  0.  1.]\n",
      "[65.  2.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  0. 35.  0.  1.]\n",
      "[55.  1.  2.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 35.  1.  0.]\n",
      "[56.  1.  1.  0.  0.  0.  0.  1.  1.  0.  1.  0.  0. 38.  1.  0.]\n",
      "[62.  1.  2.  0.  0.  1.  0.  1.  1.  0.  1.  1.  0. 41.  1.  0.]\n",
      "[50.  1.  1.  1.  0.  0.  0.  0.  1.  1.  0.  0.  0. 37.  1.  0.]\n",
      "[69.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0. 33.  1.  0.]\n",
      "[35.  1.  1.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0. 34.  1.  0.]\n",
      "[35.  1.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  1. 33.  0.  1.]\n",
      "[40.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 28.  0.  1.]\n",
      "[40.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. 30.  0.  1.]\n",
      "[56.  1.  2.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0. 33.  0.  1.]\n",
      "[40.  2.  0.  1.  0.  1.  0.  1.  1.  1.  1.  0.  0. 38.  1.  0.]\n",
      "[50.  1.  1.  1.  0.  0.  0.  0.  0.  0.  1.  0.  1. 37.  1.  0.]\n",
      "[49.  1.  1.  0.  1.  0.  1.  0.  1.  0.  0.  0.  0. 37.  1.  0.]\n",
      "[35.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 27.  1.  0.]\n",
      "[57.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[89.  1.  1.  1.  0.  1.  0.  0.  1.  1.  1.  0.  1. 44.  1.  0.]\n",
      "[38.  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  1. 33.  0.  1.]\n",
      "[40.  2.  0.  0.  0.  1.  0.  0.  1.  0.  1.  0.  0. 32.  1.  0.]\n",
      "[65.  1.  1.  1.  0.  1.  0.  0.  1.  0.  1.  0.  1. 42.  1.  0.]\n",
      "[56.  1.  0.  1.  1.  1.  0.  1.  1.  1.  1.  1.  0. 45.  1.  0.]\n",
      "[62.  2.  0.  1.  0.  0.  0.  0.  1.  0.  1.  1.  1. 37.  1.  0.]\n",
      "[49.  1.  0.  0.  0.  0.  0.  1.  0.  1.  1.  0.  1. 36.  1.  0.]\n",
      "[36.  1.  1.  0.  0.  0.  0.  1.  1.  0.  1.  0.  0. 36.  1.  0.]\n",
      "[44.  2.  0.  1.  1.  0.  0.  1.  0.  1.  0.  1.  0. 36.  1.  0.]\n",
      "[64.  2.  0.  0.  0.  0.  0.  0.  1.  1.  0.  1.  0. 33.  1.  0.]\n",
      "[43.  1.  1.  1.  0.  0.  0.  1.  1.  1.  1.  1.  0. 43.  1.  0.]\n",
      "[50.  1.  0.  1.  0.  0.  0.  0.  1.  0.  1.  1.  0. 36.  1.  0.]\n",
      "[27.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0. 30.  0.  1.]\n",
      "[39.  1.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0. 32.  0.  1.]\n",
      "[100.   2.   0.   0.   0.   0.   0.   0.   1.   0.   1.   0.   0.  31.\n",
      "   1.   0.]\n",
      "[60.  1.  1.  1.  0.  0.  0.  0.  1.  0.  1.  1.  0. 40.  1.  0.]\n",
      "[36.  2.  0.  0.  0.  0.  0.  1.  1.  0.  0.  1.  0. 31.  1.  0.]\n",
      "[65.  1.  0.  1.  1.  1.  0.  1.  1.  1.  0.  1.  0. 43.  1.  0.]\n",
      "[36.  1.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0. 31.  1.  0.]\n",
      "[65.  1.  0.  1.  0.  1.  0.  0.  1.  0.  0.  0.  0. 35.  0.  1.]\n",
      "[50.  1.  0.  0.  0.  1.  0.  1.  0.  0.  1.  0.  0. 34.  0.  1.]\n",
      "[80.  1.  2.  1.  0.  1.  1.  0.  1.  0.  0.  0.  0. 39.  1.  0.]\n",
      "[89.  1.  0.  1.  1.  1.  0.  1.  1.  0.  0.  1.  0. 41.  1.  0.]\n",
      "[67.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  1.  0. 39.  1.  0.]\n",
      "[45.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0. 30.  0.  1.]\n",
      "[50.  1.  1.  1.  1.  0.  0.  1.  1.  0.  1.  0.  0. 41.  1.  0.]\n",
      "[55.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  1.  0. 39.  0.  1.]\n",
      "[90.  2.  0.  1.  0.  0.  0.  1.  1.  1.  1.  1.  1. 41.  0.  1.]\n",
      "[56.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0. 31.  0.  1.]\n",
      "[58.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 29.  0.  1.]\n",
      "[30.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 27.  0.  1.]\n",
      "[39.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0. 34.  0.  1.]\n",
      "[70.  1.  1.  1.  0.  0.  0.  0.  0.  0.  1.  1.  0. 38.  1.  0.]\n",
      "[55.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 34.  1.  0.]\n",
      "[44.  1.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 34.  1.  0.]\n",
      "[46.  1.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 30.  1.  0.]\n",
      "[48.  2.  0.  1.  0.  0.  0.  0.  1.  1.  0.  0.  0. 32.  0.  1.]\n",
      "[35.  1.  1.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0. 36.  1.  0.]\n",
      "[60.  1.  1.  1.  0.  0.  0.  0.  1.  0.  1.  1.  0. 40.  1.  0.]\n",
      "[35.  1.  1.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0. 36.  1.  0.]\n",
      "[58.  1.  0.  1.  0.  0.  0.  1.  1.  0.  1.  1.  0. 39.  1.  0.]\n",
      "[48.  1.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0. 32.  1.  0.]\n",
      "[50.  2.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  1. 32.  1.  0.]\n",
      "[45.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 28.  0.  1.]\n",
      "[50.  1.  1.  0.  0.  1.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[49.  2.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 34.  0.  1.]\n",
      "[78.  1.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  0. 41.  0.  1.]\n",
      "[36.  2.  0.  1.  0.  0.  0.  1.  1.  0.  1.  1.  1. 37.  1.  0.]\n",
      "[57.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 34.  1.  0.]\n",
      "[65.  2.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  1. 35.  1.  0.]\n",
      "[65.  2.  0.  1.  0.  0.  0.  0.  1.  0.  1.  1.  0. 35.  0.  1.]\n",
      "[60.  1.  1.  1.  0.  0.  0.  0.  1.  0.  1.  1.  0. 40.  1.  0.]\n",
      "[58.  1.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0. 33.  0.  1.]\n",
      "[38.  1.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.  0. 33.  1.  0.]\n",
      "[53.  2.  0.  1.  1.  1.  0.  0.  1.  1.  1.  0.  0. 38.  1.  0.]\n",
      "[45.  1.  1.  0.  1.  0.  0.  1.  0.  0.  1.  1.  1. 41.  0.  1.]\n",
      "[48.  1.  1.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0. 35.  0.  1.]\n",
      "[72.  2.  0.  1.  0.  1.  0.  1.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[46.  1.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0. 32.  0.  1.]\n",
      "[59.  1.  1.  1.  0.  1.  0.  1.  1.  0.  1.  0.  0. 42.  1.  0.]\n",
      "[46.  1.  0.  1.  1.  1.  0.  0.  0.  1.  1.  1.  0. 40.  0.  1.]\n",
      "[69.  1.  2.  1.  0.  1.  0.  1.  1.  0.  1.  1.  0. 43.  1.  0.]\n",
      "[75.  2.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0. 31.  1.  0.]\n",
      "[34.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0. 34.  0.  1.]\n",
      "[58.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0. 47.  1.  0.]\n",
      "[65.  1.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0. 35.  1.  0.]\n",
      "[30.  2.  0.  1.  0.  0.  0.  1.  1.  0.  1.  0.  1. 35.  0.  1.]\n",
      "[46.  2.  0.  1.  0.  0.  0.  1.  1.  0.  1.  1.  1. 38.  1.  0.]\n",
      "[45.  2.  0.  1.  0.  1.  0.  0.  1.  0.  1.  1.  1. 38.  1.  0.]\n",
      "[40.  2.  0.  1.  0.  0.  0.  0.  0.  0.  1.  1.  0. 32.  0.  1.]\n",
      "[65.  2.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[69.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0. 33.  1.  0.]\n",
      "[45.  2.  0.  1.  1.  1.  0.  1.  1.  1.  1.  1.  0. 42.  0.  1.]\n",
      "[49.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0. 32.  1.  0.]\n",
      "[48.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. 30.  0.  1.]\n",
      "[53.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. 30.  1.  0.]\n",
      "[58.  2.  0.  1.  1.  1.  0.  1.  0.  0.  1.  1.  0. 39.  1.  0.]\n",
      "[47.  2.  0.  1.  0.  1.  0.  1.  1.  1.  1.  0.  0. 38.  1.  0.]\n",
      "[46.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0. 30.  1.  0.]\n",
      "[42.  1.  1.  0.  0.  0.  0.  0.  1.  1.  0.  0.  1. 37.  1.  0.]\n",
      "[63.  1.  2.  1.  0.  1.  0.  1.  1.  0.  1.  0.  0. 41.  0.  1.]\n",
      "[35.  2.  0.  1.  0.  1.  0.  1.  1.  0.  1.  0.  1. 37.  1.  0.]\n",
      "[50.  1.  1.  1.  0.  0.  0.  0.  1.  0.  1.  1.  0. 39.  1.  0.]\n",
      "[75.  2.  0.  1.  0.  0.  0.  1.  1.  0.  1.  1.  0. 37.  1.  0.]\n",
      "[42.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0. 35.  0.  1.]\n",
      "[43.  1.  0.  0.  1.  0.  0.  1.  1.  0.  0.  1.  0. 36.  1.  0.]\n",
      "[50.  1.  1.  1.  0.  0.  0.  0.  0.  1.  1.  0.  0. 37.  0.  1.]\n",
      "[60.  1.  2.  1.  1.  1.  0.  0.  1.  0.  1.  1.  0. 43.  1.  0.]\n",
      "[50.  1.  1.  1.  1.  1.  0.  1.  1.  0.  1.  1.  0. 45.  1.  0.]\n",
      "[48.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 32.  1.  0.]\n",
      "[60.  1.  2.  0.  0.  1.  0.  1.  1.  0.  1.  0.  0. 39.  1.  0.]\n",
      "[63.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 37.  0.  1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. 30.  0.  1.]\n",
      "[53.  1.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0. 32.  0.  1.]\n",
      "[46.  1.  2.  0.  0.  1.  0.  0.  1.  0.  1.  1.  0. 38.  1.  0.]\n",
      "[42.  1.  1.  1.  0.  0.  0.  0.  1.  1.  1.  0.  0. 39.  1.  0.]\n",
      "[60.  1.  2.  0.  1.  1.  0.  0.  1.  1.  0.  0.  0. 39.  1.  0.]\n",
      "[30.  2.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0. 31.  0.  1.]\n",
      "[65.  2.  0.  1.  1.  1.  0.  1.  1.  1.  1.  1.  0. 43.  1.  0.]\n",
      "[55.  1.  1.  0.  0.  1.  0.  0.  1.  0.  1.  0.  0. 38.  1.  0.]\n",
      "[35.  1.  1.  0.  1.  0.  1.  1.  1.  1.  1.  0.  0. 42.  1.  0.]\n",
      "[50.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1. 30.  1.  0.]\n",
      "[64.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 27.  0.  1.]\n",
      "[69.  1.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  1. 35.  1.  0.]\n",
      "[56.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0. 35.  1.  0.]\n",
      "[40.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 28.  0.  1.]\n",
      "[44.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  1. 34.  0.  1.]\n",
      "[61.  1.  0.  1.  0.  0.  0.  1.  1.  0.  1.  1.  0. 39.  0.  1.]\n",
      "[55.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0. 31.  1.  0.]\n",
      "[51.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1. 37.  1.  0.]\n",
      "[70.  1.  1.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0. 36.  0.  1.]\n",
      "[45.  2.  0.  1.  0.  1.  0.  0.  1.  0.  1.  1.  0. 36.  1.  0.]\n",
      "[33.  1.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0. 31.  0.  1.]\n",
      "[45.  2.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 32.  0.  1.]\n",
      "[56.  2.  0.  1.  1.  0.  0.  0.  1.  1.  1.  1.  0. 39.  1.  0.]\n",
      "[52.  1.  0.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0. 36.  0.  1.]\n",
      "[47.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. 33.  1.  0.]\n",
      "[45.  2.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  1. 32.  0.  1.]\n",
      "[29.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1. 30.  0.  1.]\n",
      "[62.  1.  1.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0. 38.  1.  0.]\n",
      "[33.  1.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.  0. 31.  0.  1.]\n",
      "[68.  1.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 36.  0.  1.]\n",
      "[45.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 31.  0.  1.]\n",
      "[88.  1.  2.  1.  0.  1.  1.  0.  1.  0.  1.  1.  0. 43.  1.  0.]\n",
      "[65.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[54.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.  1.  0. 41.  0.  1.]\n",
      "[57.  1.  0.  1.  0.  1.  0.  1.  1.  1.  0.  0.  0. 39.  0.  1.]\n",
      "[54.  1.  0.  1.  0.  0.  0.  0.  1.  1.  1.  0.  0. 36.  1.  0.]\n",
      "[60.  1.  2.  1.  0.  0.  0.  0.  0.  1.  1.  1.  0. 39.  1.  0.]\n",
      "[65.  1.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 38.  1.  0.]\n",
      "[55.  1.  1.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0. 42.  0.  1.]\n",
      "[47.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 28.  1.  0.]\n",
      "[65.  1.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 36.  1.  0.]\n",
      "[75.  1.  2.  1.  0.  0.  0.  0.  1.  0.  0.  1.  0. 37.  0.  1.]\n",
      "[85.  2.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 31.  1.  0.]\n",
      "[70.  2.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 31.  0.  1.]\n",
      "[70.  1.  2.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1. 41.  1.  0.]\n",
      "[27.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 26.  1.  0.]\n",
      "[31.  1.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  1. 33.  0.  1.]\n",
      "[46.  1.  1.  1.  0.  0.  0.  1.  1.  1.  0.  1.  0. 41.  1.  0.]\n",
      "[45.  2.  0.  1.  0.  0.  0.  1.  0.  1.  0.  0.  0. 32.  1.  0.]\n",
      "[84.  2.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0. 31.  0.  1.]\n",
      "[65.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0. 31.  1.  0.]\n",
      "[65.  2.  0.  1.  0.  1.  0.  0.  1.  0.  1.  1.  0. 37.  1.  0.]\n",
      "[75.  1.  0.  1.  0.  0.  0.  1.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[58.  1.  1.  1.  0.  0.  0.  0.  1.  0.  0.  1.  0. 38.  0.  1.]\n",
      "[65.  2.  0.  1.  0.  1.  0.  1.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[50.  1.  1.  1.  1.  1.  0.  1.  1.  0.  1.  1.  0. 45.  1.  0.]\n",
      "[57.  1.  1.  1.  0.  0.  0.  0.  1.  1.  1.  0.  0. 40.  1.  0.]\n",
      "[65.  2.  0.  1.  0.  1.  0.  0.  1.  0.  1.  1.  0. 37.  1.  0.]\n",
      "[80.  1.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 36.  1.  0.]\n",
      "[42.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 26.  0.  1.]\n",
      "[48.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 33.  0.  1.]\n",
      "[43.  2.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0. 30.  0.  1.]\n",
      "[61.  1.  2.  0.  0.  1.  0.  1.  1.  1.  1.  1.  1. 45.  1.  0.]\n",
      "[36.  1.  1.  1.  0.  1.  0.  1.  1.  1.  1.  1.  0. 44.  1.  0.]\n",
      "[42.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  1. 34.  0.  1.]\n",
      "[44.  2.  0.  1.  0.  1.  0.  0.  1.  1.  0.  0.  0. 34.  0.  1.]\n",
      "[50.  2.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0. 32.  0.  1.]\n",
      "[30.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 27.  0.  1.]\n",
      "[30.  2.  0.  1.  0.  0.  0.  1.  1.  0.  1.  0.  1. 35.  0.  1.]\n",
      "[82.  1.  2.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0. 35.  1.  0.]\n",
      "[53.  1.  0.  1.  1.  1.  0.  1.  1.  1.  1.  0.  1. 44.  0.  1.]\n",
      "[67.  1.  1.  1.  0.  1.  0.  0.  1.  0.  0.  0.  0. 38.  1.  0.]\n",
      "[75.  2.  0.  1.  0.  0.  0.  1.  1.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[42.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1. 32.  0.  1.]\n",
      "[52.  2.  0.  1.  0.  1.  0.  0.  1.  1.  1.  1.  0. 38.  1.  0.]\n",
      "[45.  1.  0.  1.  0.  0.  0.  0.  0.  0.  1.  1.  0. 34.  1.  0.]\n",
      "[72.  1.  2.  1.  0.  0.  0.  0.  1.  1.  1.  1.  0. 41.  0.  1.]\n",
      "[55.  1.  1.  0.  0.  0.  0.  0.  1.  0.  1.  1.  0. 38.  1.  0.]\n",
      "[50.  1.  1.  1.  0.  0.  0.  1.  1.  0.  1.  0.  0. 39.  1.  0.]\n",
      "[70.  1.  1.  1.  0.  1.  0.  0.  1.  0.  0.  1.  0. 40.  1.  0.]\n",
      "[63.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 34.  1.  0.]\n",
      "[55.  1.  1.  0.  0.  1.  0.  0.  1.  1.  1.  0.  0. 40.  1.  0.]\n",
      "[67.  1.  0.  1.  1.  0.  0.  1.  1.  1.  1.  1.  0. 43.  1.  0.]\n",
      "[55.  2.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[60.  1.  0.  1.  0.  1.  0.  1.  1.  0.  1.  0.  0. 39.  1.  0.]\n",
      "[57.  1.  2.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 33.  1.  0.]\n",
      "[38.  2.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0. 31.  0.  1.]\n",
      "[61.  1.  0.  1.  0.  1.  0.  0.  1.  1.  0.  0.  0. 37.  0.  1.]\n",
      "[47.  1.  1.  1.  0.  1.  0.  0.  1.  1.  1.  0.  0. 41.  1.  0.]\n",
      "[33.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1. 31.  1.  0.]\n",
      "[27.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 26.  0.  1.]\n",
      "[35.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 32.  1.  0.]\n",
      "[67.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  0. 48.  1.  0.]\n",
      "[40.  1.  1.  0.  0.  0.  0.  1.  1.  0.  1.  0.  0. 37.  1.  0.]\n",
      "[52.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 36.  1.  0.]\n",
      "[42.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 28.  0.  1.]\n",
      "[52.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0. 30.  0.  1.]\n",
      "[48.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 32.  1.  0.]\n",
      "[70.  2.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[26.  1.  1.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0. 33.  1.  0.]\n",
      "[41.  1.  0.  0.  0.  0.  0.  1.  1.  0.  1.  0.  1. 36.  0.  1.]\n",
      "[50.  1.  0.  1.  0.  1.  0.  1.  1.  0.  1.  0.  0. 38.  0.  1.]\n",
      "[63.  1.  1.  1.  0.  1.  0.  0.  1.  0.  0.  1.  0. 40.  0.  1.]\n",
      "[54.  2.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  1. 32.  0.  1.]\n",
      "[53.32490975  2.          0.          0.          0.          0.\n",
      "  0.          1.          1.          0.          0.          1.\n",
      "  0.         32.          0.          1.        ]\n",
      "[58.  2.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0. 29.  0.  1.]\n",
      "[50.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  1.  1. 40.  1.  0.]\n",
      "[43.  1.  1.  1.  1.  0.  0.  1.  1.  1.  1.  1.  0. 45.  1.  0.]\n",
      "[43.  1.  0.  1.  0.  0.  0.  0.  1.  1.  0.  0.  1. 36.  0.  1.]\n",
      "[58.  1.  0.  1.  0.  0.  0.  1.  0.  0.  1.  1.  0. 37.  0.  1.]\n",
      "[53.  1.  0.  1.  0.  1.  0.  1.  1.  1.  0.  0.  0. 38.  0.  1.]\n",
      "[58.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0. 34.  1.  0.]\n",
      "[60.  1.  1.  1.  0.  0.  0.  1.  1.  0.  1.  0.  0. 40.  1.  0.]\n",
      "[84.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0. 31.  0.  1.]\n",
      "[60.  2.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 31.  0.  1.]\n",
      "[70.  2.  0.  1.  0.  1.  1.  0.  1.  0.  1.  1.  0. 39.  1.  0.]\n",
      "[28.  1.  1.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[67.  1.  2.  1.  1.  0.  0.  1.  0.  0.  1.  1.  0. 41.  0.  1.]\n",
      "[70.  1.  2.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 35.  1.  0.]\n",
      "[48.  2.  0.  0.  0.  0.  0.  1.  0.  1.  0.  1.  1. 34.  0.  1.]\n",
      "[55.  1.  1.  0.  0.  0.  0.  1.  1.  0.  1.  0.  0. 38.  1.  0.]\n",
      "[87.  1.  1.  1.  0.  0.  0.  1.  1.  0.  1.  1.  0. 42.  1.  0.]\n",
      "[50.  1.  0.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0. 36.  0.  1.]\n",
      "[89.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  1.  0. 39.  1.  0.]\n",
      "[60.  2.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 31.  1.  0.]\n",
      "[45.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0. 30.  0.  1.]\n",
      "[60.  2.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0. 31.  1.  0.]\n",
      "[56.  1.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 35.  1.  0.]\n",
      "[52.  1.  0.  1.  0.  1.  0.  1.  1.  0.  0.  0.  0. 36.  1.  0.]\n",
      "[26.  2.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  1. 30.  0.  1.]\n",
      "[37.  1.  0.  1.  1.  0.  0.  0.  1.  0.  1.  0.  0. 35.  0.  1.]\n",
      "[58.  1.  2.  0.  0.  1.  0.  0.  1.  1.  1.  0.  0. 39.  1.  0.]\n",
      "[50.  2.  0.  1.  0.  1.  0.  0.  1.  0.  1.  1.  0. 36.  1.  0.]\n",
      "[50.  1.  0.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0. 34.  1.  0.]\n",
      "[54.  1.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0. 32.  0.  1.]\n",
      "[49.  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0. 32.  0.  1.]\n",
      "[27.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 26.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "for i in data_set2:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_x,actual_y = x_data_y_data_create(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actual_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actual_y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 # losses.mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold #1\n",
      "Train on 750 samples, validate on 84 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.2485 - acc: 0.5880 - val_loss: 0.2422 - val_acc: 0.5833\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.2372 - acc: 0.6040 - val_loss: 0.2425 - val_acc: 0.5833\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.2368 - acc: 0.6040 - val_loss: 0.2443 - val_acc: 0.5833\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.2367 - acc: 0.6040 - val_loss: 0.2409 - val_acc: 0.5833\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.2356 - acc: 0.6040 - val_loss: 0.2408 - val_acc: 0.5833\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.2359 - acc: 0.6040 - val_loss: 0.2402 - val_acc: 0.5833\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.2347 - acc: 0.6040 - val_loss: 0.2467 - val_acc: 0.5833\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.2350 - acc: 0.6040 - val_loss: 0.2424 - val_acc: 0.5833\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.2350 - acc: 0.6040 - val_loss: 0.2398 - val_acc: 0.5833\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.2356 - acc: 0.6040 - val_loss: 0.2405 - val_acc: 0.5833\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.2352 - acc: 0.6040 - val_loss: 0.2393 - val_acc: 0.5833\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.2340 - acc: 0.6040 - val_loss: 0.2391 - val_acc: 0.5833\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.2349 - acc: 0.6040 - val_loss: 0.2394 - val_acc: 0.5833\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.2346 - acc: 0.6040 - val_loss: 0.2396 - val_acc: 0.5833\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.2344 - acc: 0.6040 - val_loss: 0.2397 - val_acc: 0.5833\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.2347 - acc: 0.6040 - val_loss: 0.2409 - val_acc: 0.5833\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.2342 - acc: 0.6040 - val_loss: 0.2392 - val_acc: 0.5833\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.2347 - acc: 0.6040 - val_loss: 0.2385 - val_acc: 0.5833\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.2334 - acc: 0.6040 - val_loss: 0.2379 - val_acc: 0.5833\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.2339 - acc: 0.6040 - val_loss: 0.2406 - val_acc: 0.5833\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.2331 - acc: 0.6040 - val_loss: 0.2375 - val_acc: 0.5833\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.2334 - acc: 0.6040 - val_loss: 0.2383 - val_acc: 0.5833\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.2324 - acc: 0.6040 - val_loss: 0.2390 - val_acc: 0.5833\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.2330 - acc: 0.6040 - val_loss: 0.2380 - val_acc: 0.5833\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.2323 - acc: 0.6040 - val_loss: 0.2394 - val_acc: 0.5833\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.2335 - acc: 0.6040 - val_loss: 0.2364 - val_acc: 0.5833\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.2323 - acc: 0.6040 - val_loss: 0.2363 - val_acc: 0.5833\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.2311 - acc: 0.6040 - val_loss: 0.2364 - val_acc: 0.5833\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.2324 - acc: 0.6040 - val_loss: 0.2358 - val_acc: 0.5833\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.2325 - acc: 0.6040 - val_loss: 0.2356 - val_acc: 0.5833\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.2318 - acc: 0.6040 - val_loss: 0.2355 - val_acc: 0.5833\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.2313 - acc: 0.6040 - val_loss: 0.2364 - val_acc: 0.5833\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.2317 - acc: 0.6040 - val_loss: 0.2355 - val_acc: 0.5833\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.2316 - acc: 0.6053 - val_loss: 0.2399 - val_acc: 0.5833\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.2319 - acc: 0.6053 - val_loss: 0.2343 - val_acc: 0.5833\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.2315 - acc: 0.6067 - val_loss: 0.2357 - val_acc: 0.5833\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.2317 - acc: 0.6040 - val_loss: 0.2339 - val_acc: 0.5833\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.2303 - acc: 0.6093 - val_loss: 0.2342 - val_acc: 0.5833\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.2313 - acc: 0.6067 - val_loss: 0.2353 - val_acc: 0.5833\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.2292 - acc: 0.6133 - val_loss: 0.2379 - val_acc: 0.5833\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.2298 - acc: 0.6093 - val_loss: 0.2335 - val_acc: 0.5833\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.2295 - acc: 0.6120 - val_loss: 0.2352 - val_acc: 0.5833\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.2308 - acc: 0.6093 - val_loss: 0.2338 - val_acc: 0.5833\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.2287 - acc: 0.6133 - val_loss: 0.2324 - val_acc: 0.5833\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.2303 - acc: 0.6133 - val_loss: 0.2323 - val_acc: 0.5833\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.2290 - acc: 0.6080 - val_loss: 0.2326 - val_acc: 0.5833\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.2285 - acc: 0.6200 - val_loss: 0.2432 - val_acc: 0.6429\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.2305 - acc: 0.6080 - val_loss: 0.2321 - val_acc: 0.5833\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.2309 - acc: 0.6187 - val_loss: 0.2313 - val_acc: 0.5833\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.2294 - acc: 0.6120 - val_loss: 0.2327 - val_acc: 0.6310\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.2296 - acc: 0.6120 - val_loss: 0.2312 - val_acc: 0.5952\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.2291 - acc: 0.6213 - val_loss: 0.2301 - val_acc: 0.5952\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.2289 - acc: 0.6160 - val_loss: 0.2312 - val_acc: 0.5833\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.2294 - acc: 0.6267 - val_loss: 0.2432 - val_acc: 0.5833\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.2304 - acc: 0.6173 - val_loss: 0.2300 - val_acc: 0.5952\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.2269 - acc: 0.6307 - val_loss: 0.2512 - val_acc: 0.5833\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.2303 - acc: 0.6200 - val_loss: 0.2344 - val_acc: 0.5833\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.2290 - acc: 0.6133 - val_loss: 0.2295 - val_acc: 0.6071\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.2275 - acc: 0.6200 - val_loss: 0.2400 - val_acc: 0.5833\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.2286 - acc: 0.6147 - val_loss: 0.2352 - val_acc: 0.5833\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.2281 - acc: 0.6267 - val_loss: 0.2312 - val_acc: 0.5833\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.2261 - acc: 0.6240 - val_loss: 0.2334 - val_acc: 0.5833\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.2283 - acc: 0.6267 - val_loss: 0.2301 - val_acc: 0.5952\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.2287 - acc: 0.6267 - val_loss: 0.2296 - val_acc: 0.5952\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.2267 - acc: 0.6293 - val_loss: 0.2365 - val_acc: 0.5833\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.2266 - acc: 0.6173 - val_loss: 0.2276 - val_acc: 0.6190\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.2282 - acc: 0.6347 - val_loss: 0.2277 - val_acc: 0.5952\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.2278 - acc: 0.6293 - val_loss: 0.2385 - val_acc: 0.5833\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.2264 - acc: 0.6333 - val_loss: 0.2292 - val_acc: 0.5952\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.2264 - acc: 0.6387 - val_loss: 0.2263 - val_acc: 0.5952\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.2266 - acc: 0.6520 - val_loss: 0.2261 - val_acc: 0.5952\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.2250 - acc: 0.6267 - val_loss: 0.2258 - val_acc: 0.5952\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.2258 - acc: 0.6240 - val_loss: 0.2362 - val_acc: 0.5833\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.2266 - acc: 0.6333 - val_loss: 0.2262 - val_acc: 0.5952\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.2276 - acc: 0.6307 - val_loss: 0.2273 - val_acc: 0.5952\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.2242 - acc: 0.6467 - val_loss: 0.2279 - val_acc: 0.5952\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.2255 - acc: 0.6440 - val_loss: 0.2290 - val_acc: 0.5952\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.2252 - acc: 0.6427 - val_loss: 0.2367 - val_acc: 0.5833\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.2259 - acc: 0.6253 - val_loss: 0.2249 - val_acc: 0.5952\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.2262 - acc: 0.6413 - val_loss: 0.2251 - val_acc: 0.5952\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.2221 - acc: 0.6413 - val_loss: 0.2337 - val_acc: 0.5833\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.2242 - acc: 0.6227 - val_loss: 0.2279 - val_acc: 0.5952\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.2248 - acc: 0.6507 - val_loss: 0.2279 - val_acc: 0.6429\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.2252 - acc: 0.6413 - val_loss: 0.2247 - val_acc: 0.6548\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.2259 - acc: 0.6360 - val_loss: 0.2412 - val_acc: 0.5833\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.2248 - acc: 0.6347 - val_loss: 0.2238 - val_acc: 0.5952\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.2240 - acc: 0.6373 - val_loss: 0.2234 - val_acc: 0.6190\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.2237 - acc: 0.6387 - val_loss: 0.2398 - val_acc: 0.5833\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.2259 - acc: 0.6173 - val_loss: 0.2259 - val_acc: 0.6548\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.2196 - acc: 0.6493 - val_loss: 0.2239 - val_acc: 0.5952\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.2245 - acc: 0.6547 - val_loss: 0.2354 - val_acc: 0.5833\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.2239 - acc: 0.6587 - val_loss: 0.2278 - val_acc: 0.5952\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.2244 - acc: 0.6253 - val_loss: 0.2349 - val_acc: 0.6429\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.2254 - acc: 0.6400 - val_loss: 0.2266 - val_acc: 0.6071\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.2230 - acc: 0.6373 - val_loss: 0.2305 - val_acc: 0.6667\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.2230 - acc: 0.6387 - val_loss: 0.2243 - val_acc: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      " - 0s - loss: 0.2223 - acc: 0.6493 - val_loss: 0.2227 - val_acc: 0.6190\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.2247 - acc: 0.6400 - val_loss: 0.2238 - val_acc: 0.6071\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.2246 - acc: 0.6200 - val_loss: 0.2237 - val_acc: 0.6548\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.2229 - acc: 0.6480 - val_loss: 0.2255 - val_acc: 0.5952\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 39us/step\n",
      "\n",
      "-------------------------------------------------------------------------------------------          \n",
      "Fold 1 score : [0.22550403575102487, 0.5952380980764117]\n",
      " best Score: 0.5952380980764117          \n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "fold #2\n",
      "Train on 750 samples, validate on 84 samples\n",
      "Epoch 1/100\n",
      " - 0s - loss: 0.3562 - acc: 0.5400 - val_loss: 0.2284 - val_acc: 0.6190\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.2389 - acc: 0.6053 - val_loss: 0.2276 - val_acc: 0.6071\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.2355 - acc: 0.5973 - val_loss: 0.2333 - val_acc: 0.6071\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.2371 - acc: 0.5987 - val_loss: 0.2270 - val_acc: 0.6071\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.2360 - acc: 0.5987 - val_loss: 0.2278 - val_acc: 0.6071\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.2370 - acc: 0.6040 - val_loss: 0.2269 - val_acc: 0.6190\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.2375 - acc: 0.6027 - val_loss: 0.2269 - val_acc: 0.6071\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.2364 - acc: 0.6080 - val_loss: 0.2315 - val_acc: 0.6190\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.2373 - acc: 0.6067 - val_loss: 0.2501 - val_acc: 0.6071\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.2406 - acc: 0.5960 - val_loss: 0.2266 - val_acc: 0.6071\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.2365 - acc: 0.5880 - val_loss: 0.2342 - val_acc: 0.6190\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.2359 - acc: 0.6067 - val_loss: 0.2279 - val_acc: 0.6071\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.2339 - acc: 0.6107 - val_loss: 0.2283 - val_acc: 0.6071\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.2367 - acc: 0.6080 - val_loss: 0.2281 - val_acc: 0.6071\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.2368 - acc: 0.6067 - val_loss: 0.2260 - val_acc: 0.6071\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.2346 - acc: 0.6053 - val_loss: 0.2309 - val_acc: 0.6190\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.2352 - acc: 0.6120 - val_loss: 0.2276 - val_acc: 0.6190\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.2377 - acc: 0.6013 - val_loss: 0.2295 - val_acc: 0.6190\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.2347 - acc: 0.6027 - val_loss: 0.2253 - val_acc: 0.6071\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.2350 - acc: 0.6213 - val_loss: 0.2256 - val_acc: 0.6429\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.2377 - acc: 0.6027 - val_loss: 0.2251 - val_acc: 0.6190\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.2349 - acc: 0.6080 - val_loss: 0.2250 - val_acc: 0.6071\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.2350 - acc: 0.6067 - val_loss: 0.2249 - val_acc: 0.6190\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.2340 - acc: 0.6080 - val_loss: 0.2385 - val_acc: 0.6429\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.2351 - acc: 0.6053 - val_loss: 0.2300 - val_acc: 0.6429\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.2344 - acc: 0.6080 - val_loss: 0.2243 - val_acc: 0.6071\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.2325 - acc: 0.6173 - val_loss: 0.2364 - val_acc: 0.6667\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.2356 - acc: 0.6107 - val_loss: 0.2251 - val_acc: 0.6190\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.2344 - acc: 0.6160 - val_loss: 0.2241 - val_acc: 0.6071\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.2332 - acc: 0.6147 - val_loss: 0.2337 - val_acc: 0.6429\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.2325 - acc: 0.6200 - val_loss: 0.2238 - val_acc: 0.6429\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.2367 - acc: 0.5893 - val_loss: 0.2251 - val_acc: 0.6548\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.2330 - acc: 0.6160 - val_loss: 0.2245 - val_acc: 0.6310\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.2342 - acc: 0.6133 - val_loss: 0.2312 - val_acc: 0.6548\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.2346 - acc: 0.6160 - val_loss: 0.2298 - val_acc: 0.6429\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.2329 - acc: 0.6147 - val_loss: 0.2262 - val_acc: 0.6548\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.2321 - acc: 0.6107 - val_loss: 0.2313 - val_acc: 0.6667\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.2328 - acc: 0.6040 - val_loss: 0.2244 - val_acc: 0.6429\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.2337 - acc: 0.6093 - val_loss: 0.2241 - val_acc: 0.6429\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.2349 - acc: 0.6187 - val_loss: 0.2234 - val_acc: 0.6310\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.2329 - acc: 0.6053 - val_loss: 0.2230 - val_acc: 0.6071\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.2351 - acc: 0.6160 - val_loss: 0.2261 - val_acc: 0.6429\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.2338 - acc: 0.6187 - val_loss: 0.2229 - val_acc: 0.6190\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.2333 - acc: 0.6120 - val_loss: 0.2345 - val_acc: 0.6310\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.2331 - acc: 0.6093 - val_loss: 0.2237 - val_acc: 0.6429\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.2310 - acc: 0.6107 - val_loss: 0.2229 - val_acc: 0.6071\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.2329 - acc: 0.6027 - val_loss: 0.2236 - val_acc: 0.6429\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.2327 - acc: 0.6147 - val_loss: 0.2268 - val_acc: 0.6548\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.2322 - acc: 0.6067 - val_loss: 0.2217 - val_acc: 0.6071\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.2332 - acc: 0.6133 - val_loss: 0.2244 - val_acc: 0.6548\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.2328 - acc: 0.6107 - val_loss: 0.2251 - val_acc: 0.6548\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.2311 - acc: 0.6107 - val_loss: 0.2220 - val_acc: 0.6310\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.2312 - acc: 0.6107 - val_loss: 0.2269 - val_acc: 0.6429\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.2317 - acc: 0.6133 - val_loss: 0.2207 - val_acc: 0.6310\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.2331 - acc: 0.6053 - val_loss: 0.2222 - val_acc: 0.6548\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.2322 - acc: 0.6227 - val_loss: 0.2207 - val_acc: 0.6190\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.2303 - acc: 0.6240 - val_loss: 0.2216 - val_acc: 0.6071\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.2313 - acc: 0.6093 - val_loss: 0.2204 - val_acc: 0.6310\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.2276 - acc: 0.6227 - val_loss: 0.2209 - val_acc: 0.6190\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.2342 - acc: 0.6200 - val_loss: 0.2207 - val_acc: 0.6190\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.2300 - acc: 0.6240 - val_loss: 0.2218 - val_acc: 0.6429\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.2294 - acc: 0.6080 - val_loss: 0.2445 - val_acc: 0.5714\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.2314 - acc: 0.6187 - val_loss: 0.2233 - val_acc: 0.6548\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.2311 - acc: 0.6240 - val_loss: 0.2217 - val_acc: 0.6071\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.2295 - acc: 0.6107 - val_loss: 0.2201 - val_acc: 0.6548\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.2297 - acc: 0.6133 - val_loss: 0.2362 - val_acc: 0.6429\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.2311 - acc: 0.6293 - val_loss: 0.2200 - val_acc: 0.6190\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.2328 - acc: 0.6213 - val_loss: 0.2200 - val_acc: 0.6310\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.2298 - acc: 0.6240 - val_loss: 0.2355 - val_acc: 0.6548\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.2320 - acc: 0.6160 - val_loss: 0.2230 - val_acc: 0.6548\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.2275 - acc: 0.6293 - val_loss: 0.2199 - val_acc: 0.6310\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.2307 - acc: 0.6240 - val_loss: 0.2193 - val_acc: 0.6429\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.2297 - acc: 0.6133 - val_loss: 0.2236 - val_acc: 0.6786\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.2307 - acc: 0.6293 - val_loss: 0.2253 - val_acc: 0.6190\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.2280 - acc: 0.6280 - val_loss: 0.2234 - val_acc: 0.6905\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.2264 - acc: 0.6160 - val_loss: 0.2204 - val_acc: 0.6548\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.2282 - acc: 0.6267 - val_loss: 0.2326 - val_acc: 0.6190\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.2315 - acc: 0.6200 - val_loss: 0.2194 - val_acc: 0.6310\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.2280 - acc: 0.6200 - val_loss: 0.2204 - val_acc: 0.6310\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.2264 - acc: 0.6227 - val_loss: 0.2237 - val_acc: 0.6190\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.2290 - acc: 0.6240 - val_loss: 0.2188 - val_acc: 0.6548\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.2301 - acc: 0.6200 - val_loss: 0.2200 - val_acc: 0.6429\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.2285 - acc: 0.6213 - val_loss: 0.2208 - val_acc: 0.6429\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.2232 - acc: 0.6307 - val_loss: 0.2184 - val_acc: 0.6429\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0b9f5fe9e98d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     history1 = model.fit(train_x, train_y,\n\u001b[1;32m     36\u001b[0m               \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m               batch_size=10,epochs=epochs,shuffle=True,verbose=2)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mplot_fig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    215\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "num_classes = 2\n",
    "epochs = 200\n",
    "\n",
    "input_shape = len(actual_x[0])\n",
    "kfold_validation_score_store = []\n",
    "kfold_accuracy_score_store = []\n",
    "avg_cross_validation_score = 0\n",
    "n_Of_fold = 10\n",
    "kf = KFold(n_Of_fold)\n",
    "fold = 0\n",
    "score = 0\n",
    "best_score = 0.0\n",
    "terget_names = [\"NO\",\"YES\"]\n",
    "model = 0\n",
    "for train,test in kf.split(actual_x):\n",
    "    fold += 1\n",
    "    print(\"fold #{}\".format(fold))\n",
    "    train_x = actual_x[train]\n",
    "    train_y = actual_y[train]\n",
    "    test_x = actual_x[test]\n",
    "    test_y = actual_y[test]\n",
    "\n",
    "    model = Sequential([\n",
    "    Dense(8,input_shape=(input_shape,),activation=\"relu\"),\n",
    "    Dense(7,activation=\"relu\"),\n",
    "    Dense(6,activation=\"relu\"),\n",
    "    Dense(2,activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss=losses.mean_squared_error,\n",
    "                  optimizer=optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history1 = model.fit(train_x, train_y,\n",
    "              validation_data = (test_x,test_y),\n",
    "              batch_size=10,epochs=epochs,shuffle=True,verbose=2)\n",
    "    \n",
    "    plot_fig(1, history1)\n",
    "    \n",
    "    score = model.evaluate(test_x, test_y)\n",
    "\n",
    "    if(best_score<score[1]):\n",
    "        best_score = score[1]\n",
    "        best_model = model\n",
    "\n",
    "    kfold_validation_score_store.append(score)\n",
    "    kfold_accuracy_score_store.append(history1.history['acc'][epochs-1])\n",
    "\n",
    "    print(\"\\n-------------------------------------------------------------------------------------------\\\n",
    "          \\nFold {0} score : {1}\\n best Score: {2}\\\n",
    "          \\n-------------------------------------------------------------------------------------------\\n\\n\"\\\n",
    "          .format(fold,score,best_score))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy\")\n",
    "best__acc_score = 0.0\n",
    "avg_accuracy_score = 0.0\n",
    "for i in kfold_accuracy_score_store:\n",
    "    if(i>best__acc_score):\n",
    "        best__acc_score = i\n",
    "    avg_accuracy_score += i\n",
    "\n",
    "print(best__acc_score)\n",
    "print(avg_accuracy_score/len(kfold_accuracy_score_store))\n",
    "\n",
    "kfold_accuracy_score_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"validation \")\n",
    "print(best_score)\n",
    "\n",
    "#print(kfold_validation_score_store)\n",
    "\n",
    "avg_cross_validation_score = 0.0\n",
    "for i in kfold_validation_score_store:\n",
    "    avg_cross_validation_score += i[1]\n",
    "\n",
    "print(avg_cross_validation_score/len(kfold_validation_score_store))\n",
    "kfold_validation_score_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 # losses.mean_absolute_error,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_classes = 2\n",
    "epochs = 200\n",
    "\n",
    "input_shape = len(actual_x[0])\n",
    "kfold_validation_score_store = []\n",
    "kfold_accuracy_score_store = []\n",
    "avg_cross_validation_score = 0\n",
    "n_Of_fold = 10\n",
    "kf = KFold(n_Of_fold)\n",
    "fold = 0\n",
    "score = 0\n",
    "best_score = 0.0\n",
    "terget_names = [\"NO\",\"YES\"]\n",
    "model = 0\n",
    "for train,test in kf.split(actual_x):\n",
    "    fold += 1\n",
    "    print(\"fold #{}\".format(fold))\n",
    "    train_x = actual_x[train]\n",
    "    train_y = actual_y[train]\n",
    "    test_x = actual_x[test]\n",
    "    test_y = actual_y[test]\n",
    "\n",
    "    model = Sequential([\n",
    "    Dense(8,input_shape=(input_shape,),activation=\"relu\"),\n",
    "    Dense(7,activation=\"relu\"),\n",
    "    Dense(6,activation=\"relu\"),\n",
    "    Dense(2,activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss= losses.mean_absolute_error,\n",
    "                  optimizer=optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history1 = model.fit(train_x, train_y,\n",
    "              validation_data = (test_x,test_y),\n",
    "              batch_size=10,epochs=epochs,shuffle=True,verbose=2)\n",
    "    \n",
    "    plot_fig(1, history1)\n",
    "    \n",
    "    \n",
    "    score = model.evaluate(test_x, test_y)\n",
    "\n",
    "    if(best_score<score[1]):\n",
    "        best_score = score[1]\n",
    "        best_model = model\n",
    "\n",
    "    kfold_validation_score_store.append(score)\n",
    "    kfold_accuracy_score_store.append(history1.history['acc'][epochs-1])\n",
    "\n",
    "    print(\"\\n-------------------------------------------------------------------------------------------\\\n",
    "          \\nFold {0} score : {1}\\n best Score: {2}\\\n",
    "          \\n-------------------------------------------------------------------------------------------\\n\\n\"\\\n",
    "          .format(fold,score,best_score))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy\")\n",
    "best__acc_score = 0.0\n",
    "avg_accuracy_score = 0.0\n",
    "for i in kfold_accuracy_score_store:\n",
    "    if(i>best__acc_score):\n",
    "        best__acc_score = i\n",
    "    avg_accuracy_score += i\n",
    "\n",
    "print(best__acc_score)\n",
    "print(avg_accuracy_score/len(kfold_accuracy_score_store))\n",
    "\n",
    "kfold_accuracy_score_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"validation \")\n",
    "print(best_score)\n",
    "\n",
    "#print(kfold_validation_score_store)\n",
    "\n",
    "avg_cross_validation_score = 0.0\n",
    "for i in kfold_validation_score_store:\n",
    "    avg_cross_validation_score += i[1]\n",
    "\n",
    "print(avg_cross_validation_score/len(kfold_validation_score_store))\n",
    "kfold_validation_score_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 # losses.mean_absolute_percentage_error,\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_classes = 2\n",
    "epochs = 200\n",
    "\n",
    "input_shape = len(actual_x[0])\n",
    "kfold_validation_score_store = []\n",
    "kfold_accuracy_score_store = []\n",
    "avg_cross_validation_score = 0\n",
    "n_Of_fold = 10\n",
    "kf = KFold(n_Of_fold)\n",
    "fold = 0\n",
    "score = 0\n",
    "best_score = 0.0\n",
    "terget_names = [\"NO\",\"YES\"]\n",
    "model = 0\n",
    "for train,test in kf.split(actual_x):\n",
    "    fold += 1\n",
    "    print(\"fold #{}\".format(fold))\n",
    "    train_x = actual_x[train]\n",
    "    train_y = actual_y[train]\n",
    "    test_x = actual_x[test]\n",
    "    test_y = actual_y[test]\n",
    "\n",
    "    model = Sequential([\n",
    "    Dense(8,input_shape=(input_shape,),activation=\"relu\"),\n",
    "    Dense(7,activation=\"relu\"),\n",
    "    Dense(6,activation=\"relu\"),\n",
    "    Dense(2,activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss=losses.mean_absolute_percentage_error,\n",
    "                  optimizer=optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history1 = model.fit(train_x, train_y,\n",
    "              validation_data = (test_x,test_y),\n",
    "              batch_size=10,epochs=epochs,shuffle=True,verbose=2)\n",
    "    \n",
    "    plot_fig(1, history1)\n",
    "    \n",
    "    score = model.evaluate(test_x, test_y)\n",
    "\n",
    "    if(best_score<score[1]):\n",
    "        best_score = score[1]\n",
    "        best_model = model\n",
    "\n",
    "    kfold_validation_score_store.append(score)\n",
    "    kfold_accuracy_score_store.append(history1.history['acc'][epochs-1])\n",
    "\n",
    "    print(\"\\n-------------------------------------------------------------------------------------------\\\n",
    "          \\nFold {0} score : {1}\\n best Score: {2}\\\n",
    "          \\n-------------------------------------------------------------------------------------------\\n\\n\"\\\n",
    "          .format(fold,score,best_score))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy\")\n",
    "best__acc_score = 0.0\n",
    "avg_accuracy_score = 0.0\n",
    "for i in kfold_accuracy_score_store:\n",
    "    if(i>best__acc_score):\n",
    "        best__acc_score = i\n",
    "    avg_accuracy_score += i\n",
    "\n",
    "print(best__acc_score)\n",
    "print(avg_accuracy_score/len(kfold_accuracy_score_store))\n",
    "\n",
    "kfold_accuracy_score_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"validation \")\n",
    "print(best_score)\n",
    "\n",
    "#print(kfold_validation_score_store)\n",
    "\n",
    "avg_cross_validation_score = 0.0\n",
    "for i in kfold_validation_score_store:\n",
    "    avg_cross_validation_score += i[1]\n",
    "\n",
    "print(avg_cross_validation_score/len(kfold_validation_score_store))\n",
    "kfold_validation_score_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 # losses.mean_squared_logarithmic_error,\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_classes = 2\n",
    "epochs = 200\n",
    "\n",
    "input_shape = len(actual_x[0])\n",
    "kfold_validation_score_store = []\n",
    "kfold_accuracy_score_store = []\n",
    "avg_cross_validation_score = 0\n",
    "n_Of_fold = 10\n",
    "kf = KFold(n_Of_fold)\n",
    "fold = 0\n",
    "score = 0\n",
    "best_score = 0.0\n",
    "terget_names = [\"NO\",\"YES\"]\n",
    "model = 0\n",
    "for train,test in kf.split(actual_x):\n",
    "    fold += 1\n",
    "    print(\"fold #{}\".format(fold))\n",
    "    train_x = actual_x[train]\n",
    "    train_y = actual_y[train]\n",
    "    test_x = actual_x[test]\n",
    "    test_y = actual_y[test]\n",
    "\n",
    "    model = Sequential([\n",
    "    Dense(8,input_shape=(input_shape,),activation=\"relu\"),\n",
    "    Dense(7,activation=\"relu\"),\n",
    "    Dense(6,activation=\"relu\"),\n",
    "    Dense(2,activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss=losses.mean_squared_logarithmic_error,\n",
    "                  optimizer=optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history1 = model.fit(train_x, train_y,\n",
    "              validation_data = (test_x,test_y),\n",
    "              batch_size=10,epochs=epochs,shuffle=True,verbose=2)\n",
    "    \n",
    "    plot_fig(1, history1)\n",
    "    \n",
    "    score = model.evaluate(test_x, test_y)\n",
    "\n",
    "    if(best_score<score[1]):\n",
    "        best_score = score[1]\n",
    "        best_model = model\n",
    "\n",
    "    kfold_validation_score_store.append(score)\n",
    "    kfold_accuracy_score_store.append(history1.history['acc'][epochs-1])\n",
    "\n",
    "    print(\"\\n-------------------------------------------------------------------------------------------\\\n",
    "          \\nFold {0} score : {1}\\n best Score: {2}\\\n",
    "          \\n-------------------------------------------------------------------------------------------\\n\\n\"\\\n",
    "          .format(fold,score,best_score))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy\")\n",
    "best__acc_score = 0.0\n",
    "avg_accuracy_score = 0.0\n",
    "for i in kfold_accuracy_score_store:\n",
    "    if(i>best__acc_score):\n",
    "        best__acc_score = i\n",
    "    avg_accuracy_score += i\n",
    "\n",
    "print(best__acc_score)\n",
    "print(avg_accuracy_score/len(kfold_accuracy_score_store))\n",
    "\n",
    "kfold_accuracy_score_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"validation \")\n",
    "print(best_score)\n",
    "\n",
    "#print(kfold_validation_score_store)\n",
    "\n",
    "avg_cross_validation_score = 0.0\n",
    "for i in kfold_validation_score_store:\n",
    "    avg_cross_validation_score += i[1]\n",
    "\n",
    "print(avg_cross_validation_score/len(kfold_validation_score_store))\n",
    "kfold_validation_score_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 #losses.squared_hinge,\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_classes = 2\n",
    "epochs = 200\n",
    "\n",
    "input_shape = len(actual_x[0])\n",
    "kfold_validation_score_store = []\n",
    "kfold_accuracy_score_store = []\n",
    "avg_cross_validation_score = 0\n",
    "n_Of_fold = 10\n",
    "kf = KFold(n_Of_fold)\n",
    "fold = 0\n",
    "score = 0\n",
    "best_score = 0.0\n",
    "terget_names = [\"NO\",\"YES\"]\n",
    "model = 0\n",
    "for train,test in kf.split(actual_x):\n",
    "    fold += 1\n",
    "    print(\"fold #{}\".format(fold))\n",
    "    train_x = actual_x[train]\n",
    "    train_y = actual_y[train]\n",
    "    test_x = actual_x[test]\n",
    "    test_y = actual_y[test]\n",
    "\n",
    "    model = Sequential([\n",
    "    Dense(8,input_shape=(input_shape,),activation=\"relu\"),\n",
    "    Dense(7,activation=\"relu\"),\n",
    "    Dense(6,activation=\"relu\"),\n",
    "    Dense(2,activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss=losses.squared_hinge,\n",
    "                  optimizer=optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history1 = model.fit(train_x, train_y,\n",
    "              validation_data = (test_x,test_y),\n",
    "              batch_size=10,epochs=epochs,shuffle=True,verbose=2)\n",
    "    \n",
    "    plot_fig(1, history1)\n",
    "    \n",
    "    score = model.evaluate(test_x, test_y)\n",
    "\n",
    "    if(best_score<score[1]):\n",
    "        best_score = score[1]\n",
    "        best_model = model\n",
    "\n",
    "    kfold_validation_score_store.append(score)\n",
    "    kfold_accuracy_score_store.append(history1.history['acc'][epochs-1])\n",
    "\n",
    "    print(\"\\n-------------------------------------------------------------------------------------------\\\n",
    "          \\nFold {0} score : {1}\\n best Score: {2}\\\n",
    "          \\n-------------------------------------------------------------------------------------------\\n\\n\"\\\n",
    "          .format(fold,score,best_score))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy\")\n",
    "best__acc_score = 0.0\n",
    "avg_accuracy_score = 0.0\n",
    "for i in kfold_accuracy_score_store:\n",
    "    if(i>best__acc_score):\n",
    "        best__acc_score = i\n",
    "    avg_accuracy_score += i\n",
    "\n",
    "print(best__acc_score)\n",
    "print(avg_accuracy_score/len(kfold_accuracy_score_store))\n",
    "\n",
    "kfold_accuracy_score_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"validation \")\n",
    "print(best_score)\n",
    "\n",
    "#print(kfold_validation_score_store)\n",
    "\n",
    "avg_cross_validation_score = 0.0\n",
    "for i in kfold_validation_score_store:\n",
    "    avg_cross_validation_score += i[1]\n",
    "\n",
    "print(avg_cross_validation_score/len(kfold_validation_score_store))\n",
    "kfold_validation_score_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 #losses.hinge,\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_classes = 2\n",
    "epochs = 200\n",
    "\n",
    "input_shape = len(actual_x[0])\n",
    "kfold_validation_score_store = []\n",
    "kfold_accuracy_score_store = []\n",
    "avg_cross_validation_score = 0\n",
    "n_Of_fold = 10\n",
    "kf = KFold(n_Of_fold)\n",
    "fold = 0\n",
    "score = 0\n",
    "best_score = 0.0\n",
    "terget_names = [\"NO\",\"YES\"]\n",
    "model = 0\n",
    "for train,test in kf.split(actual_x):\n",
    "    fold += 1\n",
    "    print(\"fold #{}\".format(fold))\n",
    "    train_x = actual_x[train]\n",
    "    train_y = actual_y[train]\n",
    "    test_x = actual_x[test]\n",
    "    test_y = actual_y[test]\n",
    "\n",
    "    model = Sequential([\n",
    "    Dense(8,input_shape=(input_shape,),activation=\"relu\"),\n",
    "    Dense(7,activation=\"relu\"),\n",
    "    Dense(6,activation=\"relu\"),\n",
    "    Dense(2,activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss=losses.hinge,\n",
    "                  optimizer=optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history1 = model.fit(train_x, train_y,\n",
    "              validation_data = (test_x,test_y),\n",
    "              batch_size=10,epochs=epochs,shuffle=True,verbose=2)\n",
    "    \n",
    "    plot_fig(1, history1)\n",
    "    \n",
    "    score = model.evaluate(test_x, test_y)\n",
    "\n",
    "    if(best_score<score[1]):\n",
    "        best_score = score[1]\n",
    "        best_model = model\n",
    "\n",
    "    kfold_validation_score_store.append(score)\n",
    "    kfold_accuracy_score_store.append(history1.history['acc'][epochs-1])\n",
    "\n",
    "    print(\"\\n-------------------------------------------------------------------------------------------\\\n",
    "          \\nFold {0} score : {1}\\n best Score: {2}\\\n",
    "          \\n-------------------------------------------------------------------------------------------\\n\\n\"\\\n",
    "          .format(fold,score,best_score))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy\")\n",
    "best__acc_score = 0.0\n",
    "avg_accuracy_score = 0.0\n",
    "for i in kfold_accuracy_score_store:\n",
    "    if(i>best__acc_score):\n",
    "        best__acc_score = i\n",
    "    avg_accuracy_score += i\n",
    "\n",
    "print(best__acc_score)\n",
    "print(avg_accuracy_score/len(kfold_accuracy_score_store))\n",
    "\n",
    "kfold_accuracy_score_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"validation \")\n",
    "print(best_score)\n",
    "\n",
    "#print(kfold_validation_score_store)\n",
    "\n",
    "avg_cross_validation_score = 0.0\n",
    "for i in kfold_validation_score_store:\n",
    "    avg_cross_validation_score += i[1]\n",
    "\n",
    "print(avg_cross_validation_score/len(kfold_validation_score_store))\n",
    "kfold_validation_score_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 #losses.categorical_hinge,\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_classes = 2\n",
    "epochs = 200\n",
    "\n",
    "input_shape = len(actual_x[0])\n",
    "kfold_validation_score_store = []\n",
    "kfold_accuracy_score_store = []\n",
    "avg_cross_validation_score = 0\n",
    "n_Of_fold = 10\n",
    "kf = KFold(n_Of_fold)\n",
    "fold = 0\n",
    "score = 0\n",
    "best_score = 0.0\n",
    "terget_names = [\"NO\",\"YES\"]\n",
    "model = 0\n",
    "for train,test in kf.split(actual_x):\n",
    "    fold += 1\n",
    "    print(\"fold #{}\".format(fold))\n",
    "    train_x = actual_x[train]\n",
    "    train_y = actual_y[train]\n",
    "    test_x = actual_x[test]\n",
    "    test_y = actual_y[test]\n",
    "\n",
    "    model = Sequential([\n",
    "    Dense(8,input_shape=(input_shape,),activation=\"relu\"),\n",
    "    Dense(7,activation=\"relu\"),\n",
    "    Dense(6,activation=\"relu\"),\n",
    "    Dense(2,activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss=losses.categorical_hinge,\n",
    "                  optimizer=optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history1 = model.fit(train_x, train_y,\n",
    "              validation_data = (test_x,test_y),\n",
    "              batch_size=10,epochs=epochs,shuffle=True,verbose=2)\n",
    "    \n",
    "    plot_fig(1, history1)\n",
    "    \n",
    "    score = model.evaluate(test_x, test_y)\n",
    "\n",
    "    if(best_score<score[1]):\n",
    "        best_score = score[1]\n",
    "        best_model = model\n",
    "\n",
    "    kfold_validation_score_store.append(score)\n",
    "    kfold_accuracy_score_store.append(history1.history['acc'][epochs-1])\n",
    "\n",
    "    print(\"\\n-------------------------------------------------------------------------------------------\\\n",
    "          \\nFold {0} score : {1}\\n best Score: {2}\\\n",
    "          \\n-------------------------------------------------------------------------------------------\\n\\n\"\\\n",
    "          .format(fold,score,best_score))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy\")\n",
    "best__acc_score = 0.0\n",
    "avg_accuracy_score = 0.0\n",
    "for i in kfold_accuracy_score_store:\n",
    "    if(i>best__acc_score):\n",
    "        best__acc_score = i\n",
    "    avg_accuracy_score += i\n",
    "\n",
    "print(best__acc_score)\n",
    "print(avg_accuracy_score/len(kfold_accuracy_score_store))\n",
    "\n",
    "kfold_accuracy_score_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"validation \")\n",
    "print(best_score)\n",
    "\n",
    "#print(kfold_validation_score_store)\n",
    "\n",
    "avg_cross_validation_score = 0.0\n",
    "for i in kfold_validation_score_store:\n",
    "    avg_cross_validation_score += i[1]\n",
    "\n",
    "print(avg_cross_validation_score/len(kfold_validation_score_store))\n",
    "kfold_validation_score_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 #losses.logcosh,\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_classes = 2\n",
    "epochs = 200\n",
    "\n",
    "input_shape = len(actual_x[0])\n",
    "kfold_validation_score_store = []\n",
    "kfold_accuracy_score_store = []\n",
    "avg_cross_validation_score = 0\n",
    "n_Of_fold = 10\n",
    "kf = KFold(n_Of_fold)\n",
    "fold = 0\n",
    "score = 0\n",
    "best_score = 0.0\n",
    "terget_names = [\"NO\",\"YES\"]\n",
    "model = 0\n",
    "for train,test in kf.split(actual_x):\n",
    "    fold += 1\n",
    "    print(\"fold #{}\".format(fold))\n",
    "    train_x = actual_x[train]\n",
    "    train_y = actual_y[train]\n",
    "    test_x = actual_x[test]\n",
    "    test_y = actual_y[test]\n",
    "\n",
    "    model = Sequential([\n",
    "    Dense(8,input_shape=(input_shape,),activation=\"relu\"),\n",
    "    Dense(7,activation=\"relu\"),\n",
    "    Dense(6,activation=\"relu\"),\n",
    "    Dense(2,activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss=losses.logcosh,\n",
    "                  optimizer=optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history1 = model.fit(train_x, train_y,\n",
    "              validation_data = (test_x,test_y),\n",
    "              batch_size=10,epochs=epochs,shuffle=True,verbose=2)\n",
    "    \n",
    "    plot_fig(1, history1)\n",
    "    \n",
    "    score = model.evaluate(test_x, test_y)\n",
    "\n",
    "    if(best_score<score[1]):\n",
    "        best_score = score[1]\n",
    "        best_model = model\n",
    "\n",
    "    kfold_validation_score_store.append(score)\n",
    "    kfold_accuracy_score_store.append(history1.history['acc'][epochs-1])\n",
    "\n",
    "    print(\"\\n-------------------------------------------------------------------------------------------\\\n",
    "          \\nFold {0} score : {1}\\n best Score: {2}\\\n",
    "          \\n-------------------------------------------------------------------------------------------\\n\\n\"\\\n",
    "          .format(fold,score,best_score))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy\")\n",
    "best__acc_score = 0.0\n",
    "avg_accuracy_score = 0.0\n",
    "for i in kfold_accuracy_score_store:\n",
    "    if(i>best__acc_score):\n",
    "        best__acc_score = i\n",
    "    avg_accuracy_score += i\n",
    "\n",
    "print(best__acc_score)\n",
    "print(avg_accuracy_score/len(kfold_accuracy_score_store))\n",
    "\n",
    "kfold_accuracy_score_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"validation \")\n",
    "print(best_score)\n",
    "\n",
    "#print(kfold_validation_score_store)\n",
    "\n",
    "avg_cross_validation_score = 0.0\n",
    "for i in kfold_validation_score_store:\n",
    "    avg_cross_validation_score += i[1]\n",
    "\n",
    "print(avg_cross_validation_score/len(kfold_validation_score_store))\n",
    "kfold_validation_score_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09 #losses.categorical_crossentropy,\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_classes = 2\n",
    "epochs = 200\n",
    "\n",
    "input_shape = len(actual_x[0])\n",
    "kfold_validation_score_store = []\n",
    "kfold_accuracy_score_store = []\n",
    "avg_cross_validation_score = 0\n",
    "n_Of_fold = 10\n",
    "kf = KFold(n_Of_fold)\n",
    "fold = 0\n",
    "score = 0\n",
    "best_score = 0.0\n",
    "terget_names = [\"NO\",\"YES\"]\n",
    "model = 0\n",
    "for train,test in kf.split(actual_x):\n",
    "    fold += 1\n",
    "    print(\"fold #{}\".format(fold))\n",
    "    train_x = actual_x[train]\n",
    "    train_y = actual_y[train]\n",
    "    test_x = actual_x[test]\n",
    "    test_y = actual_y[test]\n",
    "\n",
    "    model = Sequential([\n",
    "    Dense(8,input_shape=(input_shape,),activation=\"relu\"),\n",
    "    Dense(7,activation=\"relu\"),\n",
    "    Dense(6,activation=\"relu\"),\n",
    "    Dense(2,activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss=losses.categorical_crossentropy,\n",
    "                  optimizer=optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history1 = model.fit(train_x, train_y,\n",
    "              validation_data = (test_x,test_y),\n",
    "              batch_size=10,epochs=epochs,shuffle=True,verbose=2)\n",
    "    \n",
    "    plot_fig(1, history1)\n",
    "    \n",
    "    score = model.evaluate(test_x, test_y)\n",
    "\n",
    "    if(best_score<score[1]):\n",
    "        best_score = score[1]\n",
    "        best_model = model\n",
    "\n",
    "    kfold_validation_score_store.append(score)\n",
    "    kfold_accuracy_score_store.append(history1.history['acc'][epochs-1])\n",
    "\n",
    "    print(\"\\n-------------------------------------------------------------------------------------------\\\n",
    "          \\nFold {0} score : {1}\\n best Score: {2}\\\n",
    "          \\n-------------------------------------------------------------------------------------------\\n\\n\"\\\n",
    "          .format(fold,score,best_score))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy\")\n",
    "best__acc_score = 0.0\n",
    "avg_accuracy_score = 0.0\n",
    "for i in kfold_accuracy_score_store:\n",
    "    if(i>best__acc_score):\n",
    "        best__acc_score = i\n",
    "    avg_accuracy_score += i\n",
    "\n",
    "print(best__acc_score)\n",
    "print(avg_accuracy_score/len(kfold_accuracy_score_store))\n",
    "\n",
    "kfold_accuracy_score_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"validation \")\n",
    "print(best_score)\n",
    "\n",
    "#print(kfold_validation_score_store)\n",
    "\n",
    "avg_cross_validation_score = 0.0\n",
    "for i in kfold_validation_score_store:\n",
    "    avg_cross_validation_score += i[1]\n",
    "\n",
    "print(avg_cross_validation_score/len(kfold_validation_score_store))\n",
    "kfold_validation_score_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 #losses.sparse_categorical_crossentropy,\n",
    "              "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_size = 10\n",
    "num_classes = 2\n",
    "epochs = 200\n",
    "\n",
    "input_shape = len(actual_x[0])\n",
    "kfold_validation_score_store = []\n",
    "kfold_accuracy_score_store = []\n",
    "avg_cross_validation_score = 0\n",
    "n_Of_fold = 10\n",
    "kf = KFold(n_Of_fold)\n",
    "fold = 0\n",
    "score = 0\n",
    "best_score = 0.0\n",
    "terget_names = [\"NO\",\"YES\"]\n",
    "model = 0\n",
    "for train,test in kf.split(actual_x):\n",
    "    fold += 1\n",
    "    print(\"fold #{}\".format(fold))\n",
    "    train_x = actual_x[train]\n",
    "    train_y = actual_y[train]\n",
    "    test_x = actual_x[test]\n",
    "    test_y = actual_y[test]\n",
    "\n",
    "    model = Sequential([\n",
    "    Dense(8,input_shape=(input_shape,),activation=\"relu\"),\n",
    "    Dense(7,activation=\"relu\"),\n",
    "    Dense(6,activation=\"relu\"),\n",
    "    Dense(2,activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss=losses.sparse_categorical_crossentropy,\n",
    "                  optimizer=optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history1 = model.fit(train_x, train_y,\n",
    "              validation_data = (test_x,test_y),\n",
    "              batch_size=10,epochs=epochs,shuffle=True,verbose=2)\n",
    "    \n",
    "    plot_fig(1, history1)\n",
    "    \n",
    "    score = model.evaluate(test_x, test_y)\n",
    "\n",
    "    if(best_score<score[1]):\n",
    "        best_score = score[1]\n",
    "        best_model = model\n",
    "\n",
    "    kfold_validation_score_store.append(score)\n",
    "    kfold_accuracy_score_store.append(history1.history['acc'][epochs-1])\n",
    "\n",
    "    print(\"\\n-------------------------------------------------------------------------------------------\\\n",
    "          \\nFold {0} score : {1}\\n best Score: {2}\\\n",
    "          \\n-------------------------------------------------------------------------------------------\\n\\n\"\\\n",
    "          .format(fold,score,best_score))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy\")\n",
    "best__acc_score = 0.0\n",
    "avg_accuracy_score = 0.0\n",
    "for i in kfold_accuracy_score_store:\n",
    "    if(i>best__acc_score):\n",
    "        best__acc_score = i\n",
    "    avg_accuracy_score += i\n",
    "\n",
    "print(best__acc_score)\n",
    "print(avg_accuracy_score/len(kfold_accuracy_score_store))\n",
    "\n",
    "kfold_accuracy_score_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"validation \")\n",
    "print(best_score)\n",
    "\n",
    "#print(kfold_validation_score_store)\n",
    "\n",
    "avg_cross_validation_score = 0.0\n",
    "for i in kfold_validation_score_store:\n",
    "    avg_cross_validation_score += i[1]\n",
    "\n",
    "print(avg_cross_validation_score/len(kfold_validation_score_store))\n",
    "kfold_validation_score_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 #losses.binary_crossentropy,\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_classes = 2\n",
    "epochs = 200\n",
    "\n",
    "input_shape = len(actual_x[0])\n",
    "kfold_validation_score_store = []\n",
    "kfold_accuracy_score_store = []\n",
    "avg_cross_validation_score = 0\n",
    "n_Of_fold = 10\n",
    "kf = KFold(n_Of_fold)\n",
    "fold = 0\n",
    "score = 0\n",
    "best_score = 0.0\n",
    "terget_names = [\"NO\",\"YES\"]\n",
    "model = 0\n",
    "for train,test in kf.split(actual_x):\n",
    "    fold += 1\n",
    "    print(\"fold #{}\".format(fold))\n",
    "    train_x = actual_x[train]\n",
    "    train_y = actual_y[train]\n",
    "    test_x = actual_x[test]\n",
    "    test_y = actual_y[test]\n",
    "\n",
    "    model = Sequential([\n",
    "    Dense(8,input_shape=(input_shape,),activation=\"relu\"),\n",
    "    Dense(7,activation=\"relu\"),\n",
    "    Dense(6,activation=\"relu\"),\n",
    "    Dense(2,activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss=losses.binary_crossentropy,\n",
    "                  optimizer=optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history1 = model.fit(train_x, train_y,\n",
    "              validation_data = (test_x,test_y),\n",
    "              batch_size=10,epochs=epochs,shuffle=True,verbose=2)\n",
    "    \n",
    "    plot_fig(1, history1)\n",
    "    \n",
    "    score = model.evaluate(test_x, test_y)\n",
    "\n",
    "    if(best_score<score[1]):\n",
    "        best_score = score[1]\n",
    "        best_model = model\n",
    "\n",
    "    kfold_validation_score_store.append(score)\n",
    "    kfold_accuracy_score_store.append(history1.history['acc'][epochs-1])\n",
    "\n",
    "    print(\"\\n-------------------------------------------------------------------------------------------\\\n",
    "          \\nFold {0} score : {1}\\n best Score: {2}\\\n",
    "          \\n-------------------------------------------------------------------------------------------\\n\\n\"\\\n",
    "          .format(fold,score,best_score))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy\")\n",
    "best__acc_score = 0.0\n",
    "avg_accuracy_score = 0.0\n",
    "for i in kfold_accuracy_score_store:\n",
    "    if(i>best__acc_score):\n",
    "        best__acc_score = i\n",
    "    avg_accuracy_score += i\n",
    "\n",
    "print(best__acc_score)\n",
    "print(avg_accuracy_score/len(kfold_accuracy_score_store))\n",
    "\n",
    "kfold_accuracy_score_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"validation \")\n",
    "print(best_score)\n",
    "\n",
    "#print(kfold_validation_score_store)\n",
    "\n",
    "avg_cross_validation_score = 0.0\n",
    "for i in kfold_validation_score_store:\n",
    "    avg_cross_validation_score += i[1]\n",
    "\n",
    "print(avg_cross_validation_score/len(kfold_validation_score_store))\n",
    "kfold_validation_score_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 #losses.kullback_leibler_divergence,\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_classes = 2\n",
    "epochs = 200\n",
    "\n",
    "input_shape = len(actual_x[0])\n",
    "kfold_validation_score_store = []\n",
    "kfold_accuracy_score_store = []\n",
    "avg_cross_validation_score = 0\n",
    "n_Of_fold = 10\n",
    "kf = KFold(n_Of_fold)\n",
    "fold = 0\n",
    "score = 0\n",
    "best_score = 0.0\n",
    "terget_names = [\"NO\",\"YES\"]\n",
    "model = 0\n",
    "for train,test in kf.split(actual_x):\n",
    "    fold += 1\n",
    "    print(\"fold #{}\".format(fold))\n",
    "    train_x = actual_x[train]\n",
    "    train_y = actual_y[train]\n",
    "    test_x = actual_x[test]\n",
    "    test_y = actual_y[test]\n",
    "\n",
    "    model = Sequential([\n",
    "    Dense(8,input_shape=(input_shape,),activation=\"relu\"),\n",
    "    Dense(7,activation=\"relu\"),\n",
    "    Dense(6,activation=\"relu\"),\n",
    "    Dense(2,activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss=losses.kullback_leibler_divergence,\n",
    "                  optimizer=optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history1 = model.fit(train_x, train_y,\n",
    "              validation_data = (test_x,test_y),\n",
    "              batch_size=10,epochs=epochs,shuffle=True,verbose=2)\n",
    "    \n",
    "    plot_fig(1, history1)\n",
    "    \n",
    "    score = model.evaluate(test_x, test_y)\n",
    "\n",
    "    if(best_score<score[1]):\n",
    "        best_score = score[1]\n",
    "        best_model = model\n",
    "\n",
    "    kfold_validation_score_store.append(score)\n",
    "    kfold_accuracy_score_store.append(history1.history['acc'][epochs-1])\n",
    "\n",
    "    print(\"\\n-------------------------------------------------------------------------------------------\\\n",
    "          \\nFold {0} score : {1}\\n best Score: {2}\\\n",
    "          \\n-------------------------------------------------------------------------------------------\\n\\n\"\\\n",
    "          .format(fold,score,best_score))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy\")\n",
    "best__acc_score = 0.0\n",
    "avg_accuracy_score = 0.0\n",
    "for i in kfold_accuracy_score_store:\n",
    "    if(i>best__acc_score):\n",
    "        best__acc_score = i\n",
    "    avg_accuracy_score += i\n",
    "\n",
    "print(best__acc_score)\n",
    "print(avg_accuracy_score/len(kfold_accuracy_score_store))\n",
    "\n",
    "kfold_accuracy_score_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"validation \")\n",
    "print(best_score)\n",
    "\n",
    "#print(kfold_validation_score_store)\n",
    "\n",
    "avg_cross_validation_score = 0.0\n",
    "for i in kfold_validation_score_store:\n",
    "    avg_cross_validation_score += i[1]\n",
    "\n",
    "print(avg_cross_validation_score/len(kfold_validation_score_store))\n",
    "kfold_validation_score_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13 #poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_classes = 2\n",
    "epochs = 200\n",
    "\n",
    "input_shape = len(actual_x[0])\n",
    "kfold_validation_score_store = []\n",
    "kfold_accuracy_score_store = []\n",
    "avg_cross_validation_score = 0\n",
    "n_Of_fold = 10\n",
    "kf = KFold(n_Of_fold)\n",
    "fold = 0\n",
    "score = 0\n",
    "best_score = 0.0\n",
    "terget_names = [\"NO\",\"YES\"]\n",
    "model = 0\n",
    "for train,test in kf.split(actual_x):\n",
    "    fold += 1\n",
    "    print(\"fold #{}\".format(fold))\n",
    "    train_x = actual_x[train]\n",
    "    train_y = actual_y[train]\n",
    "    test_x = actual_x[test]\n",
    "    test_y = actual_y[test]\n",
    "\n",
    "    model = Sequential([\n",
    "    Dense(8,input_shape=(input_shape,),activation=\"relu\"),\n",
    "    Dense(7,activation=\"relu\"),\n",
    "    Dense(6,activation=\"relu\"),\n",
    "    Dense(2,activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss=losses.poisson,\n",
    "                  optimizer=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history1 = model.fit(train_x, train_y,\n",
    "              validation_data = (test_x,test_y),\n",
    "              batch_size=10,epochs=epochs,shuffle=True,verbose=2)\n",
    "    \n",
    "    plot_fig(1, history1)\n",
    "    \n",
    "    score = model.evaluate(test_x, test_y)\n",
    "\n",
    "    if(best_score<score[1]):\n",
    "        best_score = score[1]\n",
    "        best_model = model\n",
    "\n",
    "    kfold_validation_score_store.append(score)\n",
    "    kfold_accuracy_score_store.append(history1.history['acc'][epochs-1])\n",
    "\n",
    "    print(\"\\n-------------------------------------------------------------------------------------------\\\n",
    "          \\nFold {0} score : {1}\\n best Score: {2}\\\n",
    "          \\n-------------------------------------------------------------------------------------------\\n\\n\"\\\n",
    "          .format(fold,score,best_score))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy\")\n",
    "best__acc_score = 0.0\n",
    "avg_accuracy_score = 0.0\n",
    "for i in kfold_accuracy_score_store:\n",
    "    if(i>best__acc_score):\n",
    "        best__acc_score = i\n",
    "    avg_accuracy_score += i\n",
    "\n",
    "print(best__acc_score)\n",
    "print(avg_accuracy_score/len(kfold_accuracy_score_store))\n",
    "\n",
    "kfold_accuracy_score_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"validation \")\n",
    "print(best_score)\n",
    "\n",
    "#print(kfold_validation_score_store)\n",
    "\n",
    "avg_cross_validation_score = 0.0\n",
    "for i in kfold_validation_score_store:\n",
    "    avg_cross_validation_score += i[1]\n",
    "\n",
    "print(avg_cross_validation_score/len(kfold_validation_score_store))\n",
    "kfold_validation_score_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14 #losses.cosine_proximity, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_classes = 2\n",
    "epochs = 200\n",
    "input_shape = len(actual_x[0])\n",
    "kfold_validation_score_store = []\n",
    "kfold_accuracy_score_store = []\n",
    "avg_cross_validation_score = 0\n",
    "n_Of_fold = 10\n",
    "kf = KFold(n_Of_fold)\n",
    "fold = 0\n",
    "score = 0\n",
    "best_score = 0.0\n",
    "terget_names = [\"NO\",\"YES\"]\n",
    "model = 0\n",
    "for train,test in kf.split(actual_x):\n",
    "    fold += 1\n",
    "    print(\"fold #{}\".format(fold))\n",
    "    train_x = actual_x[train]\n",
    "    train_y = actual_y[train]\n",
    "    test_x = actual_x[test]\n",
    "    test_y = actual_y[test]\n",
    "\n",
    "    model = Sequential([\n",
    "    Dense(8,input_shape=(input_shape,),activation=\"relu\"),\n",
    "    Dense(7,activation=\"relu\"),\n",
    "    Dense(6,activation=\"relu\"),\n",
    "    Dense(2,activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss=losses.cosine_proximity,\n",
    "                  optimizer=optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history1 = model.fit(train_x, train_y,\n",
    "              validation_data = (test_x,test_y),\n",
    "              batch_size=10,epochs=epochs,shuffle=True,verbose=2)\n",
    "    \n",
    "    plot_fig(1, history1)\n",
    "    \n",
    "    score = model.evaluate(test_x, test_y)\n",
    "\n",
    "    if(best_score<score[1]):\n",
    "        best_score = score[1]\n",
    "        best_model = model\n",
    "\n",
    "    kfold_validation_score_store.append(score)\n",
    "    kfold_accuracy_score_store.append(history1.history['acc'][epochs-1])\n",
    "\n",
    "    print(\"\\n-------------------------------------------------------------------------------------------\\\n",
    "          \\nFold {0} score : {1}\\n best Score: {2}\\\n",
    "          \\n-------------------------------------------------------------------------------------------\\n\\n\"\\\n",
    "          .format(fold,score,best_score))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"accuracy\")\n",
    "best__acc_score = 0.0\n",
    "avg_accuracy_score = 0.0\n",
    "for i in kfold_accuracy_score_store:\n",
    "    if(i>best__acc_score):\n",
    "        best__acc_score = i\n",
    "    avg_accuracy_score += i\n",
    "\n",
    "print(best__acc_score)\n",
    "print(avg_accuracy_score/len(kfold_accuracy_score_store))\n",
    "\n",
    "kfold_accuracy_score_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"validation \")\n",
    "print(best_score)\n",
    "\n",
    "#print(kfold_validation_score_store)\n",
    "\n",
    "avg_cross_validation_score = 0.0\n",
    "for i in kfold_validation_score_store:\n",
    "    avg_cross_validation_score += i[1]\n",
    "\n",
    "print(avg_cross_validation_score/len(kfold_validation_score_store))\n",
    "kfold_validation_score_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "batch_size = 10\n",
    "num_classes = 2\n",
    "epochs = 200\n",
    "\n",
    "input_shape = len(actual_x[0])\n",
    "kfold_validation_score_store = []\n",
    "avg_cross_validation_score = 0\n",
    "n_Of_fold = 10\n",
    "kf = KFold(n_Of_fold)\n",
    "fold = 0\n",
    "score = 0\n",
    "best_score = 0.0\n",
    "terget_names = [\"NO\",\"YES\"]\n",
    "model = 0\n",
    "\n",
    "for train,test in kf.split(actual_x):\n",
    "    fold += 1\n",
    "    print(\"\\n\\nfold #{}\".format(fold))\n",
    "    train_x = actual_x[train]\n",
    "    train_y = actual_y[train]\n",
    "    test_x = actual_x[test]\n",
    "    test_y = actual_y[test]\n",
    "\n",
    "    model = Sequential([\n",
    "    Dense(8,input_shape=(input_shape,),activation=\"relu\"),\n",
    "    Dense(7,activation=\"relu\"),\n",
    "    Dense(6,activation=\"relu\"),\n",
    "    Dense(2,activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss=losses.categorical_crossentropy,\n",
    "                  optimizer=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history1 = model.fit(train_x, train_y,\n",
    "              validation_data = (test_x,test_y),\n",
    "              batch_size=10,epochs=epochs,shuffle=True,verbose=2)\n",
    "    \n",
    "    plot_fig(1, history1)\n",
    "    \n",
    "    #model.save(\"heart_attack_risk_prediction_fold_no_\"+str(fold)+\"_with_cross_validation.h5\")\n",
    "\n",
    "    rounded_predections = model.predict_classes(test_x,batch_size=10,verbose=0)\n",
    "    rounded_predicted_result = rounded_predections\n",
    "    #pred = model.predict(self.test_x)\n",
    "    pred_class = model.predict_classes(test_x)\n",
    "    score = model.evaluate(test_x, test_y)\n",
    "\n",
    "    if(best_score<score[1]):\n",
    "        best_score = score[1]\n",
    "        best_model = model\n",
    "\n",
    "    kfold_validation_score_store.append(score)\n",
    "\n",
    "    print(\"\\nFold {0} score : {1}\\n best Score: {2}\".format(fold,score,best_score))\n",
    "\n",
    "    #print(\"\\n\\n\")\n",
    "\n",
    "    #cm = confussion_matrix_generator(test_y,rounded_predicted_result)\n",
    "    #ROC_curve_generator(model,test_x)\n",
    "\n",
    "    #print(classification_report(pred_class,test_y,target_names=terget_names))\n",
    "\n",
    "    #performance(cm)\n",
    "\n",
    "    #print(\"\\n\\nnew\\n\\n\")\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_validation_score_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
